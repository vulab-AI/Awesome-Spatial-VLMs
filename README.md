# Awesome-Spatial-VLMs
Spatial Intelligence in Vision-Language Models: A Comprehensive Survey

## Benchmark Evaluation of Spatial Vision-Language Models
Facilitating the evaluation of published spatial related benchmarks, we summarize the dataset used in the evaluation section.

The related code is stored [here](./code/evaluation/) (coming soon).



We recollect the published spatial related datasets for evaluation. The following table summarizes the key datasets used for benchmarking spatial vision-language models:

<table>
  <tr>
    <th>Dataset Name</th>
    <th>Description</th>
    <th>Link</th>
  </tr>
  <tr>
    <td>EgoOrientBench</td>
    <td>Egocentric spatial understanding benchmark</td>
    <td rowspan="10"><a href="https://huggingface.co/datasets/LLDDSS/Awesome_Spatial_VQA_Benchmarks">Link</a></td>
  </tr>
  <tr><td>GeoMeter(real)</td><td>A depth-aware spatial reasoning benchmark</td></tr>
  <tr><td>SEED-Bench (Spatial section)</td><td>Subset focusing on spatial relations</td></tr>
  <tr><td>MM-Vet (Spat)</td><td>Spatial awareness evaluation track</td></tr>
  <tr><td>Whatâ€™s Up</td><td>Spatial relation in visual grounding</td></tr>
  <tr><td>CV-Bench</td><td>Visual-center spatial benchmark</td></tr>
  <tr><td>SRBench</td><td>The extrapolation of spatial benchmark</td></tr>
  <tr><td>MindCube</td><td>The extrapolation of spatial benchmark</td></tr>
  <tr><td>OmniSpatial</td><td>Comprehensive spatial reasoning dataset</td></tr>
  <tr><td>RealWorldQA</td><td>Comprehensive spatial reasoning dataset</td></tr>
  <tr>
    <td>ViewSpatial-Bench</td>
    <td>Multi-view spatial reasoning benchmark</td>
    <td><a href="https://huggingface.co/datasets/LLDDSS/Awesome_Spatial_VQA_Benchmarks_ViewSpatial-Bench">Link</a></td>
  </tr>
</table>
