import torch
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import warnings
import numpy as np

# disable some warnings
transformers.logging.set_verbosity_error()
transformers.logging.disable_progress_bar()
warnings.filterwarnings('ignore')

# set device
assert torch.cuda.is_available(), "CUDA not available!"
device = torch.device("cuda")
print("âœ… CUDA available on:", torch.cuda.get_device_name(device))

model_name = 'RussRobin/SpatialBot-3B'
offset_bos = 0

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    trust_remote_code=True
)
model.to(device).eval()

# Force-load lazy-initialized submodules and move them to CUDA
base = model.get_model() if hasattr(model, "get_model") else model

# Dummy image to trigger init if lazy
dummy_image = Image.new('RGB', (224, 224), color=(0, 0, 0))
_ = model.process_images([dummy_image, dummy_image], model.config)

# Move mm_projector
if hasattr(base, "mm_projector") and base.mm_projector is not None:
    base.mm_projector.to(device=device, dtype=model.dtype)

# Move vision_tower
if hasattr(base, "vision_tower") and base.vision_tower is not None:
    vt = base.vision_tower[0] if isinstance(base.vision_tower, (list, tuple)) else base.vision_tower
    if hasattr(vt, "to"):
        vt.to(device)
    if hasattr(vt, "vision_tower") and hasattr(vt.vision_tower, "to"):
        vt.vision_tower.to(device)

# Move visual encoder if any
for attr in ("image_encoder", "visual_encoder", "vision_model"):
    if hasattr(base, attr) and getattr(base, attr) is not None:
        enc = getattr(base, attr)
        if hasattr(enc, "to"):
            enc.to(device)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)


def build_input_with_images(
    prompt: str,
    image_count: int,
    tokenizer,
    device,
    *,
    images_per_group: int = 2,
    line_sep: str = "\n",
    group_sep: str = "\n\n",
    offset_bos: int = 0,
) -> torch.Tensor:
    """
    å°†è‹¥å¹² <image n> å ä½ç¬¦æ’å…¥åˆ° USER æ®µè½ä¸­ï¼Œå¹¶åœ¨æ–‡æœ¬ä¸å›¾ç‰‡ token ä¹‹é—´æ­£ç¡®æ‹¼æ¥ã€‚
    å‡è®¾ç‰¹æ®Š token æ˜ å°„ä¸º: <image n> -> token_id = -(200 + n)
    ä¾‹å¦‚: <image 1> -> -201, <image 2> -> -202, ...

    å‚æ•°:
        prompt: æ–‡æœ¬æç¤º
        image_count: æ€»å›¾ç‰‡å¼ æ•° (>= 0)
        tokenizer: åˆ†è¯å™¨ï¼Œéœ€æœ‰ .__call__ è¿”å›å« input_ids
        device: torch device
        images_per_group: æ¯ç»„å¤šå°‘å¼ å›¾ç‰‡ï¼ˆé»˜è®¤ 2ï¼‰
        line_sep: ç»„å†…å ä½ç¬¦ä¹‹é—´çš„åˆ†éš”ç¬¦
        group_sep: ç»„ä¸ç»„ä¹‹é—´çš„åˆ†éš”ç¬¦
        offset_bos: å¯¹ååŠæ®µ token çš„èµ·å§‹åç§»ï¼ˆç”¨äºè·³è¿‡ BOS ç­‰ï¼‰

    è¿”å›:
        shape = (1, seq_len) çš„ LongTensorï¼ˆå·²æ”¾åˆ° deviceï¼‰
    """

    assert image_count >= 0, "image_count å¿…é¡» >= 0"
    assert images_per_group >= 1, "images_per_group å¿…é¡» >= 1"

    # 1) æ„é€ å ä½ç¬¦æ–‡æœ¬ï¼š<image 1> ... <image image_count>
    placeholders = [f"<image {i}>" for i in range(1, image_count + 1)]

    # æ ¹æ® images_per_group åˆ†ç»„å¹¶æ‹¼æ¥æ–‡æœ¬
    grouped_text_parts = []
    for start in range(0, image_count, images_per_group):
        group = placeholders[start:start + images_per_group]
        grouped_text_parts.append(line_sep.join(group))
    placeholders_block = group_sep.join(grouped_text_parts) if grouped_text_parts else ""

    # 2) æ‹¼è£…å®Œæ•´å¯¹è¯æ–‡æœ¬ï¼ˆä»…åœ¨ USER æ®µé‡Œæ”¾å ä½ç¬¦ï¼‰
    #    æ³¨æ„ï¼šå¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œplaceholders_block ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä¹Ÿèƒ½æ­£å¸¸å·¥ä½œ
    header = (
        "A chat between a curious user and an artificial intelligence assistant. "
        "The assistant gives helpful, detailed, and polite answers to the user's questions. "
    )
    user_prefix = "USER: "
    assistant_prefix = " ASSISTANT:"

    # 3) ä¸ºäº†åœ¨æ–‡æœ¬ä¸å›¾ç‰‡ token ä¹‹é—´ç²¾ç¡®æ‹¼æ¥ï¼Œæˆ‘ä»¬æŠŠå ä½ç¬¦æ•´ä½“ä½œä¸º split æ ‡è®°
    #    æ–‡æœ¬ç»“æ„: [header + user_prefix] + placeholders_block + [\n + prompt + assistant_prefix]
    before = header + user_prefix
    after = (("\n" if placeholders_block else "") + f"{prompt}{assistant_prefix}")

    # æŒ‰å ä½ç¬¦å—åˆ‡åˆ†ï¼ˆä¼šå¾—åˆ° [before, after] ä¸¤æ®µï¼‰
    # è‹¥æ²¡æœ‰å›¾ç‰‡ï¼Œå ä½ç¬¦å—ä¸ºç©ºï¼Œåˆ™æˆ‘ä»¬ç›´æ¥æŠŠå›¾ç‰‡ token æ’åœ¨ before å’Œ after ä¹‹é—´å³å¯
    text_chunks = [tokenizer(before).input_ids, tokenizer(after).input_ids]

    # 4) ç”Ÿæˆä¸ <image n> å¯¹åº”çš„ç‰¹æ®Š token åºåˆ—
    #    æ˜ å°„: n -> -(200 + n)
    image_special_tokens = [-(200 + n) for n in range(1, image_count + 1)]

    # 5) æ‹¼æ¥ input_ids
    input_ids = []
    input_ids += text_chunks[0]
    input_ids += image_special_tokens
    input_ids += text_chunks[1][offset_bos:]  # å¯é€‰è·³è¿‡ BOS

    # 6) è½¬æˆå¼ é‡
    return torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)


# def inference(prompt,image_list,depth_map_list):
#     image_num = len(image_list)
#     #get image and depth_map text prompt
#     image_text = "\n\n".join(
#     [f"<image {i}>\n<image {i+1}>" for i in range(1, 2*image_num, 2)]
# )
#     depth_list=[]
#     for depth_map in depth_map_list:
#         if len(depth_map.getbands()) == 1:
#             img = np.array(depth_map)
#             h, w = img.shape
#             rgb_depth = np.zeros((h, w, 3), dtype=np.uint8)
#             rgb_depth[:, :, 0] = (img // 1024) * 4
#             rgb_depth[:, :, 1] = (img // 32) * 8
#             rgb_depth[:, :, 2] = (img % 32) * 8
#             depth_map = Image.fromarray(rgb_depth, 'RGB')
#         depth_list.append(depth_map)

#     image_pairs_list=[[image_list[i], depth_list[i]] for i in range(image_num)]
   




# # Prompt
prompt = 'What is the depth value of point <0.5,0.2>? Answer directly from depth map.'
text = f"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <image 1>\n<image 2>\n{prompt} ASSISTANT:"
text_chunks = [tokenizer(chunk).input_ids for chunk in text.split('<image 1>\n<image 2>\n')]
input_ids = torch.tensor(
    text_chunks[0] + [-201] + [-202] + text_chunks[1][offset_bos:],
    dtype=torch.long
).unsqueeze(0).to(device)

# Load images
image1 = Image.open("rgb.jpg")
image2 = Image.open("depth.png")
print("Image sizes:", image1.size, image2.size)

# Convert grayscale depth to RGB
if len(image2.getbands()) == 1:
    img = np.array(image2)
    h, w = img.shape
    rgb_depth = np.zeros((h, w, 3), dtype=np.uint8)
    rgb_depth[:, :, 0] = (img // 1024) * 4
    rgb_depth[:, :, 1] = (img // 32) * 8
    rgb_depth[:, :, 2] = (img % 32) * 8
    image2 = Image.fromarray(rgb_depth, 'RGB')

# Preprocess images
image_tensor = model.process_images([image1, image2], model.config)
image_tensor = image_tensor.to(dtype=model.dtype, device=device)

# Confirm device states
print("ğŸ“¦ input_ids    :", input_ids.device)
print("ğŸ–¼ï¸  image_tensor :", image_tensor.device)
print("ğŸ”§ model         :", next(model.parameters()).device)
if hasattr(base, "mm_projector"):
    print("ğŸ§  mm_projector  :", next(base.mm_projector.parameters()).device)

# Generate output
with torch.inference_mode():
    output_ids = model.generate(
        input_ids=input_ids,
        images=image_tensor,
        max_new_tokens=100,
        use_cache=True,
        repetition_penalty=1.0
    )[0]

# Decode
output_text = tokenizer.decode(output_ids[input_ids.shape[1]:], skip_special_tokens=True).strip()
print("ğŸ¤– Output:", output_text)
