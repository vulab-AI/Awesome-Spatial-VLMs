{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5242bd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuo/anaconda3/envs/llm_3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"LLDDSS/Awesome_Spatial_VLMs\")\n",
    "dataset =load_dataset(\"LLDDSS/Awesome_Spatial_VQA_Benchmarks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0275d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['EgoOrientBench', 'GeoMeter_Real', 'SEED_Bench_Spatial', 'MM_Vet_Spatial', 'CV_Bench', 'Whats_Up', 'SRBench', 'MindCube', 'realworldqa', 'OmniSpatial'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.save_to_disk(\"dataset\")\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1acc11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': [{'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=480x640 at 0x7770B8729150>}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=480x640 at 0x7770993EDB50>}, {'type': 'text', 'text': 'Based on these two views showing the same scene: in which direction did I move from the first view to the second view? A. Directly left B. Diagonally forward and right C. Diagonally forward and left D. Directly right'}]}]\n",
      "([<PIL.Image.Image image mode=RGB size=476x644 at 0x776C7CCF0390>, <PIL.Image.Image image mode=RGB size=476x644 at 0x776C7CCF0590>], None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "def resize_max_800(image: Image.Image) -> Image.Image:\n",
    "    max_size = 800\n",
    "    width, height = image.size\n",
    "    # 如果最大边长已经<=1080，直接返回原图\n",
    "    if max(width, height) <= max_size:\n",
    "        return image\n",
    "\n",
    "    if width >= height:\n",
    "        new_width = max_size\n",
    "        new_height = int(height * max_size / width)\n",
    "    else:\n",
    "        new_height = max_size\n",
    "        new_width = int(width * max_size / height)\n",
    "\n",
    "    resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "    return resized_image\n",
    "\n",
    "def make_message_prompt(item):\n",
    "    image_list=['image_0','image_1','image_2','image_3']\n",
    "    message = [{\n",
    "        \"role\": \"user\",\n",
    "    }]\n",
    "    content = []\n",
    "\n",
    "    imgs=[]\n",
    "    for image_key in image_list:\n",
    "        if item[image_key] is not None:\n",
    "            image = item[image_key].convert(\"RGB\")\n",
    "            image = resize_max_800(image)\n",
    "            imgs.append(image)\n",
    "            content.append({\n",
    "                \"type\": \"image\", \n",
    "                \"image\": image\n",
    "            })\n",
    "    \n",
    "    prompt= item[\"prompt\"]\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt\n",
    "    })\n",
    "    message[0][\"content\"] = content\n",
    "    return message\n",
    "\n",
    "item= dataset['MindCube'][0]\n",
    "message = make_message_prompt(item)\n",
    "print(message)\n",
    "\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "image_input=process_vision_info(message)\n",
    "print(image_input )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f576db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuo/anaconda3/envs/llm_3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Fetching 5 files: 100%|██████████| 5/5 [01:18<00:00, 15.70s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "def make_question_prompt(image, question, options):\n",
    "\n",
    "    return (\n",
    "        f\"Answer the question based on the image:\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"{options}\\n\"\n",
    "        \"Only return the answer (a word or a phrase).\"\n",
    "    )\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "    parser.add_argument(\"--output_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=10)\n",
    "    return parser.parse_args()\n",
    "\n",
    "#make it batchsize = 50\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    #load dataset\n",
    "    dataset = load_dataset(\"LLDDSS/Awesome_Spatial_VLMs\")\n",
    "    #load model\n",
    "    processor = AutoProcessor.from_pretrained(args.model_path)\n",
    "    model = AutoModelForVision2Seq.from_pretrained(args.model_path, torch_dtype=torch.float16, use_cache=True)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    messages = []\n",
    "    id_list = []\n",
    "    for bench in dataset.keys():\n",
    "        print(\"Processing bench:\", bench)\n",
    "\n",
    "        for item in dataset[bench]:\n",
    "            image = item[\"image\"].convert(\"RGB\")\n",
    "            prompt= make_question_prompt(\n",
    "                image=image,\n",
    "                question=item[\"question\"],\n",
    "                options=item[\"options\"]\n",
    "            )\n",
    "            message = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\", \n",
    "                        \"image\": image\n",
    "                    },\n",
    "                    {   \n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }]\n",
    "            messages.append(message)\n",
    "            id_list.append({\"bench_name\": bench, \"id\": item[\"id\"], \"question\": item[\"question\"], \"options\": item[\"options\"], \"answer\": item[\"GT\"]})\n",
    "        \n",
    "        all_outputs = []\n",
    "        for i in tqdm(range(0, len(messages), args.batch_size)):\n",
    "            batch_messages = messages[i:i + args.batch_size]\n",
    "            batch_id_list = id_list[i:i + args.batch_size]\n",
    "            \n",
    "            # Preparation for inference\n",
    "            text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in batch_messages]\n",
    "            \n",
    "            image_inputs,_= process_vision_info(batch_messages)\n",
    "            inputs = processor(\n",
    "                text=text,\n",
    "                images=image_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            \n",
    "            # Inference: Generation of the output\n",
    "            generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=1024, do_sample=False)\n",
    "            \n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            batch_output_text = processor.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )\n",
    "\n",
    "            for output_text, item_id in zip(batch_output_text, batch_id_list):\n",
    "                output_text = output_text.strip()\n",
    "                all_outputs.append({\n",
    "                    \"bench_name\": item_id[\"bench_name\"],\n",
    "                    \"id\": item_id[\"id\"],\n",
    "                    \"question\": item_id[\"question\"],\n",
    "                    \"options\": item_id[\"options\"],\n",
    "                    \"GT\": item_id[\"answer\"],\n",
    "                    \"result\": output_text\n",
    "                })\n",
    "        # Save the results to a file\n",
    "        with open(f\"{args.output_path}/{bench}_results.json\", \"w\") as f:\n",
    "            json.dump(all_outputs, f, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39a95c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuo/anaconda3/envs/vilasr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-11 13:55:14 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-11 13:55:15 [utils.py:326] non-default args: {'model': 'inclusionAI/ViLaSR', 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.85, 'disable_log_stats': True, 'limit_mm_per_prompt': {'image': 10}}\n",
      "INFO 09-11 13:55:22 [__init__.py:711] Resolved architecture: Qwen2_5_VLForConditionalGeneration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-11 13:55:22 [__init__.py:2816] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 09-11 13:55:22 [__init__.py:1750] Using max model len 128000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 13:55:22,638\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-11 13:55:22 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:25 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:25 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='inclusionAI/ViLaSR', speculative_config=None, tokenizer='inclusionAI/ViLaSR', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=inclusionAI/ViLaSR, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:26 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m WARNING 09-11 13:55:26 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 17772.47it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 16384.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:30 [gpu_model_runner.py:1953] Starting to load model inclusionAI/ViLaSR...\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:30 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m WARNING 09-11 13:55:30 [cuda.py:211] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:30 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:30 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.25it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.37it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.12it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.20it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:34 [default_loader.py:262] Loading weights took 3.48 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:35 [gpu_model_runner.py:2007] Model loading took 15.6264 GiB and 3.852578 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:35 [gpu_model_runner.py:2591] Encoder cache will be initialized with a budget of 98304 tokens, and profiled with 1 video items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:43 [backends.py:548] Using cache directory: /home/tuo/.cache/vllm/torch_compile_cache/8bfd6d20d0/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:43 [backends.py:559] Dynamo bytecode transform time: 3.78 s\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.049 s\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:47 [monitor.py:34] torch.compile takes 3.78 s in total\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:49 [gpu_worker.py:276] Available KV cache memory: 22.15 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:49 [kv_cache_utils.py:849] GPU KV cache size: 414,736 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:49 [kv_cache_utils.py:853] Maximum concurrency for 128,000 tokens per request: 3.24x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:54 [gpu_model_runner.py:2708] Graph capturing finished in 5 secs, took 0.53 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=1389041)\u001b[0;0m INFO 09-11 13:55:54 [core.py:214] init engine (profile, create kv cache, warmup model) took 19.17 seconds\n",
      "INFO 09-11 13:55:55 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 15252.01it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 2744.96it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 11949.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <|im_start|>system\n",
      "You are a helpful vision-language assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>What is in this image?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 13573.80it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 11184.81it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:04<00:00,  4.37s/it]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it, est. speed input: 158.27 toks/s, output: 38.42 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Answer: The image shows an aerial view of a cityscape, prominently featuring a large, iconic monument that resembles the Arc de Triomphe in Paris. The surrounding area is filled with buildings, roads, and green spaces, typical of a well-planned urban environment. The architecture and layout suggest this is a significant historical and cultural landmark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from qwen_vl_utils import process_vision_info, fetch_image\n",
    "import os\n",
    "\n",
    "#use cuda :3\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "# ---------------------\n",
    "# 基本配置\n",
    "# ---------------------\n",
    "model_path = \"inclusionAI/ViLaSR\"   # 你要测的模型\n",
    "device_count = torch.cuda.device_count()\n",
    "\n",
    "# 初始化 vLLM 引擎\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    dtype=\"bfloat16\",\n",
    "    tensor_parallel_size=device_count,\n",
    "    limit_mm_per_prompt={\"image\": 10},   # 每个 prompt 最多带多少张图\n",
    "    gpu_memory_utilization=0.85,\n",
    ")\n",
    "\n",
    "# Processor & Tokenizer\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.padding_side = \"left\"\n",
    "processor.tokenizer = tokenizer\n",
    "\n",
    "# ---------------------\n",
    "# 构造输入\n",
    "# ---------------------\n",
    "image_path = \"temp.png\"  # 替换成你本地的图片\n",
    "question = \"What is in this image?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful vision-language assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": image_path,\n",
    "                \"max_pixels\": 256*28*28   # 可选，控制图像缩放\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": question\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 转换成模型输入\n",
    "prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n",
    "print(\"Prompt:\", prompt)\n",
    "\n",
    "llm_inputs = [{\n",
    "    \"prompt\": prompt,\n",
    "    \"prompt_token_ids\": tokenizer.encode(prompt, add_special_tokens=False),\n",
    "    \"multi_modal_data\": {\"image\": image_inputs},   # 传图像\n",
    "}]\n",
    "\n",
    "# ---------------------\n",
    "# 推理\n",
    "# ---------------------\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "outputs = llm.generate(prompts=llm_inputs, sampling_params=sampling_params)\n",
    "response = outputs[0].outputs[0].text\n",
    "\n",
    "print(\"Model Answer:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469895c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89d12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vilasr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
