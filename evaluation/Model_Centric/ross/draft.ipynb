{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a28bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tuo/anaconda3/envs/ross/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading RossQwen2ForCausalLM ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.90s/it]\n",
      "Some weights of the model checkpoint at HaochenWang/ross-qwen2-7b were not used when initializing RossQwen2ForCausalLM: ['model.mm_inv_projector.net.blocks.0.adaLN_modulation.1.bias', 'model.mm_inv_projector.net.blocks.0.adaLN_modulation.1.weight', 'model.mm_inv_projector.net.blocks.0.attn.proj.bias', 'model.mm_inv_projector.net.blocks.0.attn.proj.weight', 'model.mm_inv_projector.net.blocks.0.attn.qkv.bias', 'model.mm_inv_projector.net.blocks.0.attn.qkv.weight', 'model.mm_inv_projector.net.blocks.0.mlp.fc1.bias', 'model.mm_inv_projector.net.blocks.0.mlp.fc1.weight', 'model.mm_inv_projector.net.blocks.0.mlp.fc2.bias', 'model.mm_inv_projector.net.blocks.0.mlp.fc2.weight', 'model.mm_inv_projector.net.blocks.1.adaLN_modulation.1.bias', 'model.mm_inv_projector.net.blocks.1.adaLN_modulation.1.weight', 'model.mm_inv_projector.net.blocks.1.attn.proj.bias', 'model.mm_inv_projector.net.blocks.1.attn.proj.weight', 'model.mm_inv_projector.net.blocks.1.attn.qkv.bias', 'model.mm_inv_projector.net.blocks.1.attn.qkv.weight', 'model.mm_inv_projector.net.blocks.1.mlp.fc1.bias', 'model.mm_inv_projector.net.blocks.1.mlp.fc1.weight', 'model.mm_inv_projector.net.blocks.1.mlp.fc2.bias', 'model.mm_inv_projector.net.blocks.1.mlp.fc2.weight', 'model.mm_inv_projector.net.blocks.2.adaLN_modulation.1.bias', 'model.mm_inv_projector.net.blocks.2.adaLN_modulation.1.weight', 'model.mm_inv_projector.net.blocks.2.attn.proj.bias', 'model.mm_inv_projector.net.blocks.2.attn.proj.weight', 'model.mm_inv_projector.net.blocks.2.attn.qkv.bias', 'model.mm_inv_projector.net.blocks.2.attn.qkv.weight', 'model.mm_inv_projector.net.blocks.2.mlp.fc1.bias', 'model.mm_inv_projector.net.blocks.2.mlp.fc1.weight', 'model.mm_inv_projector.net.blocks.2.mlp.fc2.bias', 'model.mm_inv_projector.net.blocks.2.mlp.fc2.weight', 'model.mm_inv_projector.net.final_layer.adaLN_modulation.1.bias', 'model.mm_inv_projector.net.final_layer.adaLN_modulation.1.weight', 'model.mm_inv_projector.net.final_layer.linear.bias', 'model.mm_inv_projector.net.final_layer.linear.weight', 'model.mm_inv_projector.net.pos_embed', 'model.mm_inv_projector.net.t_embedder.mlp.0.bias', 'model.mm_inv_projector.net.t_embedder.mlp.0.weight', 'model.mm_inv_projector.net.t_embedder.mlp.2.bias', 'model.mm_inv_projector.net.t_embedder.mlp.2.weight', 'model.mm_inv_projector.net.x_embedder.proj.bias', 'model.mm_inv_projector.net.x_embedder.proj.weight', 'model.mm_inv_projector.net.z_embedder.1.bias', 'model.mm_inv_projector.net.z_embedder.1.weight', 'model.pixel_decoder.pixel_decoder.decoder.conv_in.bias', 'model.pixel_decoder.pixel_decoder.decoder.conv_in.weight', 'model.pixel_decoder.pixel_decoder.decoder.conv_norm_out.bias', 'model.pixel_decoder.pixel_decoder.decoder.conv_norm_out.weight', 'model.pixel_decoder.pixel_decoder.decoder.conv_out.bias', 'model.pixel_decoder.pixel_decoder.decoder.conv_out.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.group_norm.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.group_norm.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_k.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_k.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_out.0.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_out.0.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_q.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_q.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_v.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_v.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.upsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.upsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.upsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.upsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv_shortcut.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv_shortcut.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.upsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.upsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv_shortcut.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv_shortcut.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.conv_in.bias', 'model.pixel_decoder.pixel_decoder.encoder.conv_in.weight', 'model.pixel_decoder.pixel_decoder.encoder.conv_norm_out.bias', 'model.pixel_decoder.pixel_decoder.encoder.conv_norm_out.weight', 'model.pixel_decoder.pixel_decoder.encoder.conv_out.bias', 'model.pixel_decoder.pixel_decoder.encoder.conv_out.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.downsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.downsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.downsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.downsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv_shortcut.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv_shortcut.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.downsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.downsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv_shortcut.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv_shortcut.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.group_norm.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.group_norm.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_k.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_k.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_out.0.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_out.0.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_q.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_q.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_v.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_v.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.norm2.weight']\n",
      "- This IS expected if you are initializing RossQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RossQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 142.50 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 144.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m      9\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaochenWang/ross-qwen2-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m tokenizer, model, image_processor, context_len \u001b[38;5;241m=\u001b[39m load_pretrained_model(\n\u001b[1;32m     12\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[1;32m     13\u001b[0m     model_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mget_model_name_from_path(model_path)\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     20\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ross/lib/python3.10/site-packages/accelerate/big_modeling.py:457\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ross/lib/python3.10/site-packages/transformers/modeling_utils.py:2875\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` or `8-bit` quantized models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2873\u001b[0m     )\n\u001b[1;32m   2874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ross/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ross/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ross/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ross/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ross/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/ross/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 142.50 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 144.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from ross.model.builder import load_pretrained_model\n",
    "from ross.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "\n",
    "from ross.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "\n",
    "\n",
    "model_path = \"HaochenWang/ross-qwen2-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path)\n",
    ")\n",
    "\n",
    "# model.cuda()\n",
    "# model.eval()\n",
    "\n",
    "image = Image.open(\"test.jpg\")\n",
    "prompt = \"who is she?\"\n",
    "\n",
    "images_tensor = process_images(\n",
    "    [image],\n",
    "    image_processor,\n",
    "    model.config,\n",
    ").cuda()\n",
    "\n",
    "input_ids = tokenizer_image_token(\n",
    "    prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\",\n",
    ").unsqueeze(0).cuda()\n",
    "\n",
    "# with torch.inference_mode():\n",
    "#     output_ids = model.generate(\n",
    "#                 input_ids,\n",
    "#         images=images_tensor,\n",
    "#         max_new_tokens=1024,\n",
    "#         num_beams=5,\n",
    "#         do_sample= False)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.7,\n",
    "        top_k=20,\n",
    "        num_beams=5,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3148d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset =load_dataset(\"LLDDSS/Awesome_Spatial_VQA_Benchmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5b9cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'image_0': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x70CBBDD3FFA0>, 'image_1': None, 'image_2': None, 'image_3': None, 'prompt': 'How many organs are in the image? Select from the following choices.\\n(A) 3\\n(B) 2\\n(C) 1\\n(D) 0', 'options': '3; 2; 1; 0', 'GT': 'C'}\n"
     ]
    }
   ],
   "source": [
    "bench=dataset['CV_Bench']\n",
    "\n",
    "print(bench[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15bca039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading RossQwen2ForCausalLM ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n",
      "Some weights of the model checkpoint at HaochenWang/ross-qwen2-7b were not used when initializing RossQwen2ForCausalLM: ['model.mm_inv_projector.net.blocks.0.adaLN_modulation.1.bias', 'model.mm_inv_projector.net.blocks.0.adaLN_modulation.1.weight', 'model.mm_inv_projector.net.blocks.0.attn.proj.bias', 'model.mm_inv_projector.net.blocks.0.attn.proj.weight', 'model.mm_inv_projector.net.blocks.0.attn.qkv.bias', 'model.mm_inv_projector.net.blocks.0.attn.qkv.weight', 'model.mm_inv_projector.net.blocks.0.mlp.fc1.bias', 'model.mm_inv_projector.net.blocks.0.mlp.fc1.weight', 'model.mm_inv_projector.net.blocks.0.mlp.fc2.bias', 'model.mm_inv_projector.net.blocks.0.mlp.fc2.weight', 'model.mm_inv_projector.net.blocks.1.adaLN_modulation.1.bias', 'model.mm_inv_projector.net.blocks.1.adaLN_modulation.1.weight', 'model.mm_inv_projector.net.blocks.1.attn.proj.bias', 'model.mm_inv_projector.net.blocks.1.attn.proj.weight', 'model.mm_inv_projector.net.blocks.1.attn.qkv.bias', 'model.mm_inv_projector.net.blocks.1.attn.qkv.weight', 'model.mm_inv_projector.net.blocks.1.mlp.fc1.bias', 'model.mm_inv_projector.net.blocks.1.mlp.fc1.weight', 'model.mm_inv_projector.net.blocks.1.mlp.fc2.bias', 'model.mm_inv_projector.net.blocks.1.mlp.fc2.weight', 'model.mm_inv_projector.net.blocks.2.adaLN_modulation.1.bias', 'model.mm_inv_projector.net.blocks.2.adaLN_modulation.1.weight', 'model.mm_inv_projector.net.blocks.2.attn.proj.bias', 'model.mm_inv_projector.net.blocks.2.attn.proj.weight', 'model.mm_inv_projector.net.blocks.2.attn.qkv.bias', 'model.mm_inv_projector.net.blocks.2.attn.qkv.weight', 'model.mm_inv_projector.net.blocks.2.mlp.fc1.bias', 'model.mm_inv_projector.net.blocks.2.mlp.fc1.weight', 'model.mm_inv_projector.net.blocks.2.mlp.fc2.bias', 'model.mm_inv_projector.net.blocks.2.mlp.fc2.weight', 'model.mm_inv_projector.net.final_layer.adaLN_modulation.1.bias', 'model.mm_inv_projector.net.final_layer.adaLN_modulation.1.weight', 'model.mm_inv_projector.net.final_layer.linear.bias', 'model.mm_inv_projector.net.final_layer.linear.weight', 'model.mm_inv_projector.net.pos_embed', 'model.mm_inv_projector.net.t_embedder.mlp.0.bias', 'model.mm_inv_projector.net.t_embedder.mlp.0.weight', 'model.mm_inv_projector.net.t_embedder.mlp.2.bias', 'model.mm_inv_projector.net.t_embedder.mlp.2.weight', 'model.mm_inv_projector.net.x_embedder.proj.bias', 'model.mm_inv_projector.net.x_embedder.proj.weight', 'model.mm_inv_projector.net.z_embedder.1.bias', 'model.mm_inv_projector.net.z_embedder.1.weight', 'model.pixel_decoder.pixel_decoder.decoder.conv_in.bias', 'model.pixel_decoder.pixel_decoder.decoder.conv_in.weight', 'model.pixel_decoder.pixel_decoder.decoder.conv_norm_out.bias', 'model.pixel_decoder.pixel_decoder.decoder.conv_norm_out.weight', 'model.pixel_decoder.pixel_decoder.decoder.conv_out.bias', 'model.pixel_decoder.pixel_decoder.decoder.conv_out.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.group_norm.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.group_norm.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_k.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_k.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_out.0.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_out.0.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_q.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_q.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_v.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.attentions.0.to_v.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.mid_block.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.resnets.2.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.upsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.0.upsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.resnets.2.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.upsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.1.upsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv_shortcut.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.conv_shortcut.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.resnets.2.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.upsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.2.upsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv_shortcut.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.conv_shortcut.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.conv1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.conv1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.conv2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.conv2.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.norm1.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.norm1.weight', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.norm2.bias', 'model.pixel_decoder.pixel_decoder.decoder.up_blocks.3.resnets.2.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.conv_in.bias', 'model.pixel_decoder.pixel_decoder.encoder.conv_in.weight', 'model.pixel_decoder.pixel_decoder.encoder.conv_norm_out.bias', 'model.pixel_decoder.pixel_decoder.encoder.conv_norm_out.weight', 'model.pixel_decoder.pixel_decoder.encoder.conv_out.bias', 'model.pixel_decoder.pixel_decoder.encoder.conv_out.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.downsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.downsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.0.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.downsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.downsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv_shortcut.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.conv_shortcut.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.1.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.downsamplers.0.conv.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.downsamplers.0.conv.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv_shortcut.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.conv_shortcut.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.2.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.down_blocks.3.resnets.1.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.group_norm.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.group_norm.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_k.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_k.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_out.0.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_out.0.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_q.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_q.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_v.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.attentions.0.to_v.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.0.norm2.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.conv1.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.conv1.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.conv2.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.conv2.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.norm1.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.norm1.weight', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.norm2.bias', 'model.pixel_decoder.pixel_decoder.encoder.mid_block.resnets.1.norm2.weight']\n",
      "- This IS expected if you are initializing RossQwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RossQwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151645\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from ross.model.builder import load_pretrained_model\n",
    "from ross.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "\n",
    "from ross.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "\n",
    "\n",
    "model_path = \"HaochenWang/ross-qwen2-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path)\n",
    ")\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa30a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ross",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
