{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/disheng/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.07it/s]\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from PIL import Image\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
        "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "pipe(text=messages)\n",
        "\n",
        "def load_and_resize_image(image_path, max_size=1000):\n",
        "    \"\"\"\n",
        "    ÂÆâÂÖ®Âä†ËΩΩÂõæÁâáÔºåËá™Âä®Ê£ÄÊü•„ÄÅÈôçÂàÜËæ®Áéá„ÄÇ\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): ÂõæÁâáË∑ØÂæÑ\n",
        "        max_size (int): Ê®°ÂûãÊîØÊåÅÁöÑÊúÄÂ§ßÂàÜËæ®Áéá (shorter edge)\n",
        "    \n",
        "    Returns:\n",
        "        PIL.Image: RGB ÂõæÁâáÔºåÂ∑≤ resize\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"‚ùå Image not found: {image_path}\")\n",
        "\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Â¶ÇÊûúÂõæÁâáÂ§™Â§ßÂàôÁ≠âÊØîÁº©Êîæ\n",
        "    width, height = img.size\n",
        "    if max(width, height) > max_size:\n",
        "        # Á≠âÊØîÁº©ÊîæÔºåÊúÄÂ§ßËæπ= max_size\n",
        "        scale = max_size / max(width, height)\n",
        "        new_size = (int(width * scale), int(height * scale))\n",
        "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EgoOrientBench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_base_path = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/\"\n",
        "json_file = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/benchmark.json\"\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "with open(json_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "import os\n",
        "\n",
        "def evaluate_spaceom(pipe, data):\n",
        "    acc = 0\n",
        "    total = len(data)\n",
        "    results = []\n",
        "    for item in tqdm(data):\n",
        "        image_path = f\"{image_base_path}/{item['image']}\"\n",
        "        # img = Image.open(image_path).convert(\"RGB\")\n",
        "        img = load_and_resize_image(image_path, max_size=224)  # Ensure image is resized correctly\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": item[\"question\"]}\n",
        "                ]\n",
        "            },\n",
        "        ]\n",
        "        response = pipe(text=messages)\n",
        "        model_answer = response[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "        results.append({\n",
        "            \"image\": item[\"image\"],\n",
        "            \"question\": item[\"question\"],\n",
        "            \"answer\": model_answer,\n",
        "            \"label\": item[\"label\"],\n",
        "            \"Accuracy\": item[\"label\"] in model_answer\n",
        "        })\n",
        "    #save results to a JSON file\n",
        "    with open(\"EgoOrientBench_spaceom_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"Accuracy: {sum(1 for r in results if r['Accuracy']) / total * 100:.2f}%\")\n",
        "    print(f\"Total: {total}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_spaceom(pipe, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 33460\n"
          ]
        }
      ],
      "source": [
        "from joblib import Parallel, delayed\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# Êï∞ÊçÆË∑ØÂæÑ\n",
        "image_base_path = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/\"\n",
        "json_file = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/benchmark.json\"\n",
        "\n",
        "with open(json_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"Total samples: {len(data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pipe():\n",
        "    from transformers import pipeline\n",
        "    pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\",\n",
        "        device=0  # Ê≥®ÊÑèËøôÈáå‰∏ÄÂÆöÂÜô device=0ÔºåÂõ†‰∏∫ÊØè‰∏™ËøõÁ®ãÈÉΩÂè™ÁúãÂà∞Ëá™Â∑±ÁöÑ GPU\n",
        "    )\n",
        "    return pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected GPUs: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.86it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.66it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.75it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.48it/s]\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "GPU 1:   0%|          | 4/8365 [00:01<44:14,  3.15it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "GPU 0:   0%|          | 10/8365 [00:01<16:19,  8.53it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "GPU 1:   0%|          | 10/8365 [00:02<31:01,  4.49it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "GPU 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8365/8365 [11:11<00:00, 12.45it/s]\n",
            "GPU 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8365/8365 [11:17<00:00, 12.34it/s]\n",
            "GPU 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8365/8365 [16:00<00:00,  8.71it/s]\n",
            "GPU 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 8364/8365 [36:01<00:00,  3.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 12.45%\n",
            "Total: 33460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8365/8365 [36:01<00:00,  3.87it/s]\n"
          ]
        }
      ],
      "source": [
        "def chunkify(lst, n):\n",
        "    \"\"\"ÊääÂàóË°®ÂùáÂàÜ‰∏∫ n ‰ªΩ\"\"\"\n",
        "    return [lst[i::n] for i in range(n)]\n",
        "\n",
        "def load_and_resize_image(path, max_size=224):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    img.thumbnail((max_size, max_size))\n",
        "    return img\n",
        "\n",
        "\n",
        "def worker(gpu_id, data_chunk):\n",
        "    import os\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "    import torch\n",
        "\n",
        "    # TODO: Âú®ËøôÈáåÈáçÊñ∞Âä†ËΩΩ pipe\n",
        "    # ‰æãÂ¶ÇÔºö\n",
        "    # from transformers import pipeline\n",
        "    # pipe = pipeline(\"your-task\", device=0)\n",
        "    pipe = load_pipe()  # ‰Ω†ÈúÄË¶ÅËá™Â∑±ÂÆûÁé∞Ëøô‰∏™\n",
        "\n",
        "    results = []\n",
        "    for item in tqdm(data_chunk, desc=f\"GPU {gpu_id}\"):\n",
        "        image_path = f\"{image_base_path}/{item['image']}\"\n",
        "        img = load_and_resize_image(image_path, max_size=224)\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": item[\"question\"]}\n",
        "                ]\n",
        "            },\n",
        "        ]\n",
        "        response = pipe(text=messages)\n",
        "        model_answer = response[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "        results.append({\n",
        "            \"image\": item[\"image\"],\n",
        "            \"question\": item[\"question\"],\n",
        "            \"answer\": model_answer,\n",
        "            \"label\": item[\"label\"],\n",
        "            \"Accuracy\": item[\"label\"] in model_answer\n",
        "        })\n",
        "    return results\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"Detected GPUs: {num_gpus}\")\n",
        "\n",
        "# ÂùáÂåÄÂàÜÂùó\n",
        "chunks = chunkify(data, num_gpus)\n",
        "\n",
        "# Âπ∂Ë°åÊâßË°å\n",
        "all_results = Parallel(n_jobs=num_gpus)(\n",
        "    delayed(worker)(gpu_id, chunk) for gpu_id, chunk in enumerate(chunks)\n",
        ")\n",
        "\n",
        "# Â±ïÂπ≥\n",
        "flat_results = [item for sublist in all_results for item in sublist]\n",
        "\n",
        "# ‰øùÂ≠ò\n",
        "with open(\"EgoOrientBench_spaceom_results.json\", \"w\") as f:\n",
        "    json.dump(flat_results, f, indent=4)\n",
        "\n",
        "acc = sum(1 for r in flat_results if r['Accuracy']) / len(flat_results)\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Total: {len(flat_results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 40.82%\n"
          ]
        }
      ],
      "source": [
        "path = \"/home/disheng/Spatial_Survey/Spatial_VLM_Survey/code/evaluation/EgoOrientBench_spaceom_results.json\"\n",
        "data = json.load(open(path, \"r\"))\n",
        "acc  = 0\n",
        "for item in data:\n",
        "    answer = item[\"answer\"].lower()\n",
        "    label = item[\"label\"].lower()\n",
        "    if label in answer:\n",
        "        acc += 1\n",
        "print(f\"Accuracy: {acc / len(data) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GeoMeter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### real data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 16.25it/s]\n",
            "Device set to use cuda:2\n",
            "Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:18<00:00,  5.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ñ∂Ô∏è Overall Accuracy: 43.00%\n",
            "number of accurate samples: 43\n",
            "number of samples: 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# üöÄ 1) ÁéØÂ¢ÉÂáÜÂ§á\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# %%\n",
        "# ‚öôÔ∏è 2) ÂèÇÊï∞ÈÖçÁΩÆ\n",
        "IMAGE_BASE   = Path(\"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Real/\")\n",
        "JSONL_FILE   = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Real/depth_height_1000_realworld.jsonl\"\n",
        "MODEL_NAME   = \"remyxai/SpaceOm\"\n",
        "TASK         = \"image-text-to-text\"\n",
        "DEVICE_ID    = 2      # ÂçïÂç°Â∞±Áî® 0\n",
        "BATCH_SIZE   = 1     # Ê†πÊçÆÊòæÂ≠òË∞É\n",
        "\n",
        "# %%\n",
        "# üìñ 3) ËØªÊï∞ÊçÆ\n",
        "data = []\n",
        "with open(JSONL_FILE, \"r\") as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        assert \"images\" in item and \"query_text\" in item and \"target_text\" in item\n",
        "        data.append(item)\n",
        "\n",
        "# %%\n",
        "# üîß 4) ÂàùÂßãÂåñ pipeline\n",
        "pipe = pipeline(\n",
        "    TASK,\n",
        "    model=MODEL_NAME,\n",
        "    device=DEVICE_ID,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# %%\n",
        "# üèÉ 5) ÊâπÈáèÊé®ÁêÜ + ËøõÂ∫¶Êù°\n",
        "results = []\n",
        "for i in tqdm(range(0, len(data), BATCH_SIZE), desc=\"Inference\"):\n",
        "    batch = data[i : i + BATCH_SIZE]\n",
        "    messages = []\n",
        "    for item in batch:\n",
        "        img = Image.open(IMAGE_BASE / item[\"images\"][0]).convert(\"RGB\")\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img},\n",
        "                {\"type\": \"text\",  \"text\":  item[\"query_text\"]}\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    outputs = pipe(text=messages)\n",
        "\n",
        "    for item, out in zip(batch, outputs):\n",
        "        # Âèñ generated_text Ëøô‰∏™ listÔºåÁÑ∂ÂêéÊâæÂá∫ assistant ÈÇ£Êù°\n",
        "        gen_list = out.get(\"generated_text\", [])\n",
        "        assistant_entry = next(\n",
        "            (entry for entry in gen_list if entry.get(\"role\")==\"assistant\"),\n",
        "            None\n",
        "        )\n",
        "        if assistant_entry is None:\n",
        "            # ‰∏á‰∏ÄÊ≤°ÊâæÂà∞ÔºåÂ∞±ÈôçÁ∫ßÂ§ÑÁêÜ\n",
        "            raw_text = str(gen_list)\n",
        "        else:\n",
        "            raw_text = assistant_entry.get(\"content\", \"\")\n",
        "\n",
        "        # Áî®Ê≠£ÂàôÂéªÊéâÊú´Â∞æÂ§ö‰ΩôÁöÑÈÄóÂè∑„ÄÅÂè•Âè∑\n",
        "        pred = re.sub(r\"[Ôºå,\\.„ÄÇ]+$\", \"\", raw_text).strip()\n",
        "\n",
        "        results.append({\n",
        "            \"image\": item[\"images\"][0],\n",
        "            \"query\": item[\"query_text\"],\n",
        "            \"pred\":  pred,\n",
        "            \"gold\":  item[\"target_text\"]\n",
        "        })\n",
        "\n",
        "# %%\n",
        "# üìä 6) ËÆ°ÁÆóÂáÜÁ°ÆÁéá & ‰øùÂ≠ò\n",
        "accuracy = 0.0\n",
        "df = pd.DataFrame(results)\n",
        "for pred, gold in zip(df[\"pred\"], df[\"gold\"]):\n",
        "    if pred.lower() in gold.lower():\n",
        "        accuracy += 1\n",
        "accuracy /= len(df)\n",
        "print(f\"‚ñ∂Ô∏è Overall Accuracy: {accuracy:.2%}\")\n",
        "print(\"number of accurate samples:\", int(accuracy * len(df)))\n",
        "print(\"number of samples:\", len(df))\n",
        "\n",
        "# df.to_json(\"geobench_real_results_with_predictions.json\",\n",
        "#            orient=\"records\", indent=2, force_ascii=False)\n",
        "# print(\"‚úÖ ÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞ geobench_real_results_with_predictions.{json,csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### synthetic data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# depth\n",
        "\n",
        "base_ir = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/depth\"\n",
        "dir_list = os.listdir(base_ir)\n",
        "\n",
        "depth_results = {}\n",
        "depth_3_shapes = {}\n",
        "depth_5_shapes = {}\n",
        "for each_sub_set in dir_list: # [images-3-shapes, images-5-shapes]\n",
        "    each_shape_set_results = {}\n",
        "    \n",
        "    for each_example in os.listdir(os.path.join(base_ir, each_sub_set, \"prompts\")):  #each prompt\n",
        "        prompts_json = os.path.join(base_ir, each_sub_set, \"prompts\", each_example)\n",
        "        with open(prompts_json, \"r\") as f:\n",
        "            prompts = json.load(f)\n",
        "\n",
        "        image_name = prompts[\"filename\"]\n",
        "        image_path = os.path.join(base_ir, each_sub_set, \"imgs\", image_name)\n",
        "        image_labelled_path = os.path.join(base_ir, each_sub_set, \"labelled\", image_name)\n",
        "        image_labelled_id_path = os.path.join(base_ir, each_sub_set, \"labelled_id\", image_name)\n",
        "        image_labelled_id_reverse_path = os.path.join(base_ir, each_sub_set, \"labelled_id_reverse\", image_name)\n",
        "\n",
        "        prompts_plain = prompts[\"prompts\"]\n",
        "        prompts_labelled = prompts[\"prompts_labelled\"]\n",
        "        prompts_labelled_id = prompts[\"prompts_labelled_id\"]\n",
        "\n",
        "        plain_results = {}\n",
        "        labelled_results = {}\n",
        "        labelled_id_results = {}\n",
        "        labelled_id_reverse_results = {}\n",
        "\n",
        "        for index, each_prompt in enumerate(prompts_plain):\n",
        "            answer = each_prompt[\"answer\"]\n",
        "            textual_prompt = each_prompt[\"prompt\"] \n",
        "            answer_set = \";\\n\".join(each_prompt[\"answerSet\"])\n",
        "\n",
        "            full_prompt = textual_prompt + \"\\nAnswer Set:\\n\" + answer_set\n",
        "\n",
        "            messages = []\n",
        "            img = load_and_resize_image(image_path)\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\",  \"text\":  full_prompt}\n",
        "                ]\n",
        "            })\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "\n",
        "            plain_results[index] = {\n",
        "                \"image\": image_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "        \n",
        "        for index, each_prompt in enumerate(prompts_labelled):\n",
        "            answer = each_prompt[\"answer\"]\n",
        "            textual_prompt = each_prompt[\"prompt\"] \n",
        "            answer_set = \";\\n\".join(each_prompt[\"answerSet\"])\n",
        "\n",
        "            full_prompt = textual_prompt + \"\\nAnswer Set:\\n\" + answer_set\n",
        "\n",
        "            messages = []\n",
        "            img = load_and_resize_image(image_labelled_path)\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\",  \"text\":  full_prompt}\n",
        "                ]\n",
        "            })\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"] \n",
        "\n",
        "            labelled_results[index] = {\n",
        "                \"image\": image_labelled_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "\n",
        "        for index, each_prompt in enumerate(prompts_labelled_id):\n",
        "            answer = each_prompt[\"answer\"]\n",
        "            textual_prompt = each_prompt[\"prompt\"] \n",
        "            answer_set = \";\\n\".join(each_prompt[\"answerSet\"])\n",
        "\n",
        "            full_prompt = textual_prompt + \"\\nAnswer Set:\\n\" + answer_set\n",
        "\n",
        "            messages = []\n",
        "            img = load_and_resize_image(image_labelled_id_path)\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\",  \"text\":  full_prompt}\n",
        "                ]\n",
        "            })\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]     \n",
        "            labelled_id_results[index] = {\n",
        "                \"image\": image_labelled_id_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "\n",
        "        for index, each_prompt in enumerate(prompts_labelled_id):\n",
        "            answer = each_prompt[\"answerReverse\"]\n",
        "            textual_prompt = each_prompt[\"prompt\"] \n",
        "            answer_set = \";\\n\".join(each_prompt[\"answerSet\"])\n",
        "\n",
        "            full_prompt = textual_prompt + \"\\nAnswer Set:\\n\" + answer_set\n",
        "\n",
        "            messages = []\n",
        "            img = load_and_resize_image(image_labelled_id_reverse_path)\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\",  \"text\":  full_prompt}\n",
        "                ]\n",
        "            })\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "            labelled_id_reverse_results[index] = {\n",
        "                \"image\": image_labelled_id_reverse_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "        if \"3-shapes\" in each_sub_set:\n",
        "            depth_3_shapes[each_sub_set] = {\n",
        "                \"plain\": plain_results,\n",
        "                \"labelled\": labelled_results,\n",
        "                \"labelled_id\": labelled_id_results,\n",
        "                \"labelled_id_reverse\": labelled_id_reverse_results\n",
        "            }\n",
        "        elif \"5-shapes\" in each_sub_set:\n",
        "            depth_5_shapes[each_sub_set] = {\n",
        "                \"plain\": plain_results,\n",
        "                \"labelled\": labelled_results,\n",
        "                \"labelled_id\": labelled_id_results,\n",
        "                \"labelled_id_reverse\": labelled_id_reverse_results\n",
        "            }   \n",
        "\n",
        "    depth_results[each_sub_set] = {\n",
        "        \"plain\": plain_results, \n",
        "        \"labelled\": labelled_results,\n",
        "        \"labelled_id\": labelled_id_results,\n",
        "        \"labelled_id_reverse\": labelled_id_reverse_results\n",
        "    }\n",
        "\n",
        "# Save results          \n",
        "import json\n",
        "depth_results_path = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/depth/depth_results.json\"\n",
        "with open(depth_results_path, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"depth_3_shapes\": depth_3_shapes,\n",
        "        \"depth_5_shapes\": depth_5_shapes,\n",
        "        \"depth_results\": depth_results\n",
        "    }, f, indent=4)     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_resize_image(path, max_size=224):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    img.thumbnail((max_size, max_size))\n",
        "    return img\n",
        "\n",
        "def chunkify(lst, n):\n",
        "    return [lst[i::n] for i in range(n)]\n",
        "\n",
        "def load_pipe():\n",
        "    from transformers import pipeline\n",
        "    pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\",\n",
        "    device=0  # Ê≥®ÊÑèËøôÈáå‰∏ÄÂÆöÂÜô device=0ÔºåÂõ†‰∏∫ÊØè‰∏™ËøõÁ®ãÈÉΩÂè™ÁúãÂà∞Ëá™Â∑±ÁöÑ GPU\n",
        "    )\n",
        "    return pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 4 GPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.59it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.88it/s]\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Subset images-5-shapes:   0%|          | 0/400 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Subset images-3-shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [09:03<00:00,  5.43s/it] \n",
            "Subset images-5-shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [1:01:12<00:00,  9.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to: /home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/depth/depth_results.json\n"
          ]
        }
      ],
      "source": [
        "def process_subset(subset_name):\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    pipe = load_pipe()  # Âè™Âú®ËøôÈáåÂä†ËΩΩÔºåÊØè‰∏™ GPU ‰∏Ä‰ªΩ\n",
        "\n",
        "    each_shape_set_results = {}\n",
        "\n",
        "    prompts_dir = os.path.join(base_ir, subset_name, \"prompts\")\n",
        "    examples = os.listdir(prompts_dir)\n",
        "\n",
        "    for each_example in tqdm(examples, desc=f\"Subset {subset_name}\"):\n",
        "        with open(os.path.join(prompts_dir, each_example), \"r\") as f:\n",
        "            prompts = json.load(f)\n",
        "\n",
        "        img_base = os.path.join(base_ir, subset_name)\n",
        "        paths = {\n",
        "            \"plain\": os.path.join(img_base, \"imgs\", prompts[\"filename\"]),\n",
        "            \"labelled\": os.path.join(img_base, \"labelled\", prompts[\"filename_labelled\"]),\n",
        "            \"labelled_id\": os.path.join(img_base, \"labelled_id\", prompts[\"filename_labelled\"]),\n",
        "            \"labelled_reverse_id\": os.path.join(img_base, \"labelled_reverse_id\", prompts[\"filename_labelled\"])\n",
        "        }\n",
        "\n",
        "        def run_prompts(prompts_list, img_path, is_reverse=False):\n",
        "            output = {}\n",
        "            for index, p in enumerate(prompts_list):\n",
        "                answer = p[\"answerReverse\"] if is_reverse else p[\"answer\"]\n",
        "                full_prompt = p[\"prompt\"] + \"\\nAnswer Set:\\n\" + \";\\n\".join(p[\"answerSet\"])\n",
        "                img = load_and_resize_image(img_path)\n",
        "\n",
        "                messages = [{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"image\", \"image\": img},\n",
        "                        {\"type\": \"text\", \"text\": full_prompt}\n",
        "                    ]\n",
        "                }]\n",
        "                try:\n",
        "                    inference_result = pipe(text=messages)\n",
        "                    final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "                    final_answer = \"ERROR\"\n",
        "\n",
        "                output[index] = {\n",
        "                    \"image\": img_path,\n",
        "                    \"prompt\": full_prompt,\n",
        "                    \"answer\": answer,\n",
        "                    \"pred\": final_answer,\n",
        "                    \"accuracy\": answer.lower() in final_answer.lower()\n",
        "                }\n",
        "            return output\n",
        "\n",
        "        plain = run_prompts(prompts[\"prompts\"], paths[\"plain\"])\n",
        "        labelled = run_prompts(prompts[\"prompts_labelled\"], paths[\"labelled\"])\n",
        "        labelled_id = run_prompts(prompts[\"prompts_labelled_id\"], paths[\"labelled_id\"])\n",
        "        labelled_reverse_id = run_prompts(prompts[\"prompts_labelled_id\"], paths[\"labelled_reverse_id\"], is_reverse=True)\n",
        "\n",
        "        each_shape_set_results[each_example] = {\n",
        "            \"plain\": plain,\n",
        "            \"labelled\": labelled,\n",
        "            \"labelled_id\": labelled_id,\n",
        "            \"labelled_reverse_id\": labelled_reverse_id\n",
        "        }\n",
        "\n",
        "    return (subset_name, each_shape_set_results)\n",
        "\n",
        "\n",
        "def main_parallel():\n",
        "    num_gpus = 4  # ‰Ω†Êúâ 4 Âº† GPU\n",
        "    print(f\"Using {num_gpus} GPUs\")\n",
        "\n",
        "    chunks = chunkify(dir_list, num_gpus)\n",
        "\n",
        "    def worker(gpu_id, chunk):\n",
        "        import os\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "        results = {}\n",
        "        for subset in chunk:\n",
        "            name, res = process_subset(subset)\n",
        "            results[name] = res\n",
        "        return results\n",
        "\n",
        "    from joblib import Parallel, delayed\n",
        "\n",
        "    all_results = Parallel(n_jobs=num_gpus)(\n",
        "        delayed(worker)(gpu_id, chunk) for gpu_id, chunk in enumerate(chunks)\n",
        "    )\n",
        "\n",
        "    # ÂêàÂπ∂\n",
        "    depth_results = {}\n",
        "    for part in all_results:\n",
        "        depth_results.update(part)\n",
        "\n",
        "    depth_3_shapes = {k: v for k, v in depth_results.items() if \"3-shapes\" in k}\n",
        "    depth_5_shapes = {k: v for k, v in depth_results.items() if \"5-shapes\" in k}\n",
        "\n",
        "    save_path = os.path.join(base_ir, \"depth_results.json\")\n",
        "    with open(save_path, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"depth_3_shapes\": depth_3_shapes,\n",
        "            \"depth_5_shapes\": depth_5_shapes,\n",
        "            \"depth_results\": depth_results\n",
        "        }, f, indent=4)\n",
        "\n",
        "    print(f\"Saved to: {save_path}\")\n",
        "\n",
        "main_parallel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 34.85%\n",
            "Accuracy: 28.84%\n",
            "number of Accurate:  4811\n",
            "number of Total:  16684\n"
          ]
        }
      ],
      "source": [
        "path = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/depth/depth_results.json\"\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "depth_results = json.load(open(path, \"r\"))\n",
        "depth_3_shapes = depth_results[\"depth_3_shapes\"]\n",
        "depth_5_shapes = depth_results[\"depth_5_shapes\"]\n",
        "depth_results = depth_results[\"depth_results\"]\n",
        "acc = 0\n",
        "total = 0\n",
        "for json in depth_3_shapes[\"images-3-shapes\"]:\n",
        "    plain = depth_3_shapes[\"images-3-shapes\"][json][\"plain\"]\n",
        "    labelled = depth_3_shapes[\"images-3-shapes\"][json][\"labelled\"]\n",
        "    labelled_id = depth_3_shapes[\"images-3-shapes\"][json][\"labelled_id\"]\n",
        "    labelled_reverse_id = depth_3_shapes[\"images-3-shapes\"][json][\"labelled_reverse_id\"]\n",
        "    acc += sum(1 for v in plain.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled_id.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled_reverse_id.values() if v[\"accuracy\"])\n",
        "    total += len(plain) + len(labelled) + len(labelled_id) + len(labelled_reverse_id)\n",
        "print(f\"Accuracy: {acc / total * 100:.2f}%\")\n",
        "\n",
        "for json in depth_5_shapes[\"images-5-shapes\"]:\n",
        "    plain = depth_5_shapes[\"images-5-shapes\"][json][\"plain\"]\n",
        "    labelled = depth_5_shapes[\"images-5-shapes\"][json][\"labelled\"]\n",
        "    labelled_id = depth_5_shapes[\"images-5-shapes\"][json][\"labelled_id\"]\n",
        "    labelled_reverse_id = depth_5_shapes[\"images-5-shapes\"][json][\"labelled_reverse_id\"]\n",
        "    acc += sum(1 for v in plain.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled_id.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled_reverse_id.values() if v[\"accuracy\"])\n",
        "    total += len(plain) + len(labelled) + len(labelled_id) + len(labelled_reverse_id)\n",
        "print(f\"Accuracy: {acc / total * 100:.2f}%\")\n",
        "print(\"number of Accurate: \", acc)\n",
        "print(\"number of Total: \", total)\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### heigh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "base_path = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/height\"\n",
        "sub_classes = os.listdir(base_path)\n",
        "\n",
        "height_results = {}\n",
        "\n",
        "print(f\"Found {len(sub_classes)} sub-classes: {sub_classes}\")\n",
        "\n",
        "for each_sub_class in sub_classes:\n",
        "    print(f\"Processing {each_sub_class}...\")\n",
        "    sub_class_path = os.path.join(base_path, each_sub_class)\n",
        "    prompts_dir = os.path.join(sub_class_path, \"prompts\")\n",
        "    prompts_files = os.listdir(prompts_dir)\n",
        "\n",
        "    for each_prompt_file in tqdm(prompts_files, desc=f\"{each_sub_class} prompts\"):\n",
        "        prompt_path = os.path.join(prompts_dir, each_prompt_file)\n",
        "        with open(prompt_path, \"r\") as f:\n",
        "            prompt_json = json.load(f)\n",
        "\n",
        "        image_name = prompt_json[\"filename\"]\n",
        "        image_path = os.path.join(sub_class_path, \"imgs\", image_name)\n",
        "        prompts_list = prompt_json[\"prompts\"]\n",
        "\n",
        "        for index, prompt in enumerate(prompts_list):\n",
        "            answer = prompt[\"answer\"]\n",
        "            answer_set = \";\\n\".join(prompt[\"answerSet\"])\n",
        "            full_prompt = prompt[\"prompt\"] + \"\\nAnswer Set:[\\n\" + answer_set + \"\\n]\"\n",
        "\n",
        "            img = load_and_resize_image(image_path)\n",
        "            messages = [{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": full_prompt}\n",
        "                ]\n",
        "            }]\n",
        "\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "\n",
        "            # ËÆ∞ÂΩïÁªìÊûúÔºåÂàÜÂ±ÇÁªìÊûÑ: {sub_class: {image_name: {index: ...}}}\n",
        "            height_results.setdefault(each_sub_class, {})\n",
        "            height_results[each_sub_class].setdefault(image_name, {})\n",
        "            height_results[each_sub_class][image_name][index] = {\n",
        "                \"image\": image_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "\n",
        "# ‰øùÂ≠òÁªìÊûú\n",
        "height_results_path = os.path.join(base_path, \"height_results.json\")\n",
        "with open(height_results_path, \"w\") as f:\n",
        "    json.dump(height_results, f, indent=4)\n",
        "\n",
        "print(f\"Results saved to: {height_results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8 sub-classes: ['images-3-stacks-colored', 'images-3-stacks-stepped-colored', 'images-3-stacks-stepped', 'images-5-stacks-stepped', 'images-5-stacks-colored', 'images-3-stacks', 'images-5-stacks', 'images-5-stacks-stepped-colored']\n",
            "Detected GPUs: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.36it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.82it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.41it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.71it/s]\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Sub-class: images-3-stacks-stepped:   2%|‚ñè         | 4/200 [00:01<01:09,  2.81it/s].60it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Sub-class: images-5-stacks-stepped:   1%|          | 2/200 [00:01<02:41,  1.22it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Sub-class: images-3-stacks-stepped-colored: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [01:02<00:00,  4.01it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 18.09it/s]\n",
            "Sub-class: images-5-stacks-stepped:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 96/200 [01:03<01:09,  1.49it/s]Device set to use cuda:0\n",
            "Sub-class: images-3-stacks-stepped: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:22<00:00,  2.43it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 17.18it/s]0,  1.47it/s]\n",
            "Sub-class: images-3-stacks:  22%|‚ñà‚ñà‚ñé       | 45/200 [00:20<01:30,  1.72it/s]Device set to use cuda:0\n",
            "Sub-class: images-5-stacks-stepped: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:11<00:00,  1.52it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 17.44it/s]t/s]\n",
            "Sub-class: images-3-stacks-colored:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 238/270 [02:12<00:18,  1.76it/s]Device set to use cuda:0\n",
            "Sub-class: images-3-stacks-colored: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 270/270 [02:30<00:00,  1.79it/s]38it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 17.52it/s]it/s]:54,  1.30it/s]\n",
            "Sub-class: images-5-stacks-stepped-colored:  10%|‚ñà         | 25/250 [00:17<02:38,  1.42it/s]Device set to use cuda:0\n",
            "Sub-class: images-3-stacks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:31<00:00,  2.17it/s]:28,  1.47it/s]\n",
            "Sub-class: images-5-stacks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:24<00:00,  1.39it/s].30it/s].30it/s]\n",
            "Sub-class: images-5-stacks-stepped-colored: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [03:00<00:00,  1.38it/s]\n",
            "Sub-class: images-5-stacks-colored: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 249/250 [02:59<00:00,  1.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/height/height_results.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sub-class: images-5-stacks-colored: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [03:00<00:00,  1.39it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import torch\n",
        "\n",
        "# Ë∑ØÂæÑ\n",
        "base_path = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/height\"\n",
        "sub_classes = os.listdir(base_path)\n",
        "print(f\"Found {len(sub_classes)} sub-classes: {sub_classes}\")\n",
        "\n",
        "def chunkify(lst, n):\n",
        "    \"\"\"Â∞ÜÂàóË°®ÂùáÂåÄÂàÜÂùó\"\"\"\n",
        "    return [lst[i::n] for i in range(n)]\n",
        "\n",
        "def process_height_sub_class(sub_class):\n",
        "    from tqdm.auto import tqdm\n",
        "    import os\n",
        "\n",
        "    pipe = load_pipe()  # ÊØèÂº† GPU Áã¨Á´ãÂä†ËΩΩ\n",
        "\n",
        "    sub_class_path = os.path.join(base_path, sub_class)\n",
        "    prompts_dir = os.path.join(sub_class_path, \"prompts\")\n",
        "    prompt_files = os.listdir(prompts_dir)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for each_prompt_file in tqdm(prompt_files, desc=f\"Sub-class: {sub_class}\"):\n",
        "        with open(os.path.join(prompts_dir, each_prompt_file), \"r\") as f:\n",
        "            prompt_json = json.load(f)\n",
        "\n",
        "        image_name = prompt_json[\"filename\"]\n",
        "        image_path = os.path.join(sub_class_path, \"imgs\", image_name)\n",
        "        prompts_list = prompt_json[\"prompts\"]\n",
        "\n",
        "        for index, prompt in enumerate(prompts_list):\n",
        "            answer = prompt[\"answer\"]\n",
        "            answer_set = \";\\n\".join(prompt[\"answerSet\"])\n",
        "            full_prompt = prompt[\"prompt\"] + \"\\nAnswer Set:[\\n\" + answer_set + \"\\n]\\nJust select the answer from the set without any explanation.\"\n",
        "\n",
        "            img = load_and_resize_image(image_path)\n",
        "            messages = [{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": full_prompt}\n",
        "                ]\n",
        "            }]\n",
        "\n",
        "            try:\n",
        "                inference_result = pipe(text=messages)\n",
        "                final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                final_answer = \"ERROR\"\n",
        "\n",
        "            # ÁªìÊûÑ: {sub_class: {image_name: {index: ...}}}\n",
        "            results.setdefault(sub_class, {})\n",
        "            results[sub_class].setdefault(image_name, {})\n",
        "            results[sub_class][image_name][index] = {\n",
        "                \"image\": image_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "def worker(gpu_id, chunk):\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "    gpu_results = {}\n",
        "\n",
        "    for sub_class in chunk:\n",
        "        res = process_height_sub_class(sub_class)\n",
        "        gpu_results.update(res)\n",
        "\n",
        "    return gpu_results\n",
        "\n",
        "def run_height_parallel():\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Detected GPUs: {num_gpus}\")\n",
        "\n",
        "    chunks = chunkify(sub_classes, num_gpus)\n",
        "\n",
        "    all_results = Parallel(n_jobs=num_gpus)(\n",
        "        delayed(worker)(gpu_id, chunk) for gpu_id, chunk in enumerate(chunks)\n",
        "    )\n",
        "\n",
        "    # ÂêàÂπ∂\n",
        "    height_results = {}\n",
        "    for part in all_results:\n",
        "        height_results.update(part)\n",
        "\n",
        "    # ‰øùÂ≠ò\n",
        "    save_path = os.path.join(base_path, \"height_results.json\")\n",
        "    with open(save_path, \"w\") as f:\n",
        "        json.dump(height_results, f, indent=4)\n",
        "\n",
        "    print(f\"Results saved to: {save_path}\")\n",
        "\n",
        "\n",
        "run_height_parallel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 24.23%\n",
            "number of accurate results: 1713\n",
            "total number of results: 7069\n"
          ]
        }
      ],
      "source": [
        "acc = 0\n",
        "total = 0\n",
        "height_results_path = os.path.join(base_path, \"height_results.json\")\n",
        "height_results = json.load(open(height_results_path, \"r\"))\n",
        "for sub_class, images in height_results.items():\n",
        "    for image_name, prompts in images.items():\n",
        "        for index, result in prompts.items():\n",
        "            if result[\"accuracy\"]:\n",
        "                acc += 1\n",
        "            total += 1\n",
        "print(f\"Accuracy: {acc / total * 100:.2f}%\")\n",
        "print(\"number of accurate results:\", acc)\n",
        "print(\"total number of results:\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final Accuracy:  27.53112815997988\n"
          ]
        }
      ],
      "source": [
        "number_of_samples = 7069 + 100 + 16684\n",
        "number_of_accurate_samples = 1713 + 4811 + 43\n",
        "print(\"final Accuracy: \", number_of_accurate_samples / number_of_samples * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OmniSpatial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# export HF_ENDPOINT=\"https://hf-mirror.com\"\n",
        "!mkdir -p dataset\n",
        "!huggingface-cli download --resume-download qizekun/OmniSpatial --local-dir dataset --repo-type dataset\n",
        "!find dataset/ -name '*.zip' -exec unzip -o {} -d dataset/ \\;\n",
        "!rm -f dataset/*.zip && rm -rf dataset/__MACOSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!export CUDA_VISIBLE_DEVICES=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def load_and_resize_image(image_path, max_size=448):\n",
        "    \"\"\"\n",
        "    ÂÆâÂÖ®Âä†ËΩΩÂõæÁâáÔºåËá™Âä®Ê£ÄÊü•„ÄÅÈôçÂàÜËæ®Áéá„ÄÇ\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): ÂõæÁâáË∑ØÂæÑ\n",
        "        max_size (int): Ê®°ÂûãÊîØÊåÅÁöÑÊúÄÂ§ßÂàÜËæ®Áéá (shorter edge)\n",
        "    \n",
        "    Returns:\n",
        "        PIL.Image: RGB ÂõæÁâáÔºåÂ∑≤ resize\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"‚ùå Image not found: {image_path}\")\n",
        "\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Â¶ÇÊûúÂõæÁâáÂ§™Â§ßÂàôÁ≠âÊØîÁº©Êîæ\n",
        "    width, height = img.size\n",
        "    if max(width, height) > max_size:\n",
        "        # Á≠âÊØîÁº©ÊîæÔºåÊúÄÂ§ßËæπ= max_size\n",
        "        scale = max_size / max(width, height)\n",
        "        new_size = (int(width * scale), int(height * scale))\n",
        "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 13.64it/s]\n",
            "Device set to use cuda:0\n",
            "  1%|          | 10/1533 [00:01<03:29,  7.27it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            " 36%|‚ñà‚ñà‚ñà‚ñå      | 551/1533 [01:47<05:12,  3.14it/s]/home/disheng/miniconda3/lib/python3.12/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1533/1533 [04:14<00:00,  6.02it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from PIL import Image\n",
        "# pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "question_types =  {0:\"Dynamic_Reasoning\", \n",
        "                   1:\"Spatial_Interaction\", \n",
        "                   2:\"Complex_Logic\", \n",
        "                   3:\"Perspective_Taking\"}\n",
        "annotation = \"/home/disheng/Spatial_Survey/Datasets/OmniSpatial/dataset/data.json\"\n",
        "import json\n",
        "annotation_data = json.load(open(annotation, \"r\"))\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "record = {}\n",
        "for item in tqdm(annotation_data):\n",
        "    iid = item[\"id\"]\n",
        "    image_id = item[\"id\"].split(\"_\")[0]\n",
        "    question_type = item[\"task_type\"]\n",
        "\n",
        "    question = item[\"question\"]\n",
        "    options = item[\"options\"]\n",
        "    full_prompt = f\"Question: {question}\\nOptions: {\"; \".join(options)}. Please only retuen a correct option without analysis.\"\n",
        "    label = item[\"answer\"]\n",
        "    if len(options) != 0:\n",
        "        label = options[label]\n",
        "    image_path = f\"/home/disheng/Spatial_Survey/Datasets/OmniSpatial/dataset/{question_type}/{image_id}.png\"\n",
        "    img = load_and_resize_image(image_path, max_size=448)  # ‰ΩøÁî®ÂÆâÂÖ®Âä†ËΩΩÂáΩÊï∞\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img},\n",
        "                {\"type\": \"text\", \"text\": full_prompt}\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    try:\n",
        "        response = pipe(text=messages)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {iid}: {e}\")\n",
        "        print(image_path)\n",
        "        print(options)\n",
        "        print(full_prompt)\n",
        "        print(label)\n",
        "        print(messages)\n",
        "        print()\n",
        "        continue\n",
        "    model_answer = response[0][\"generated_text\"][-1][\"content\"]\n",
        "    # record all of information for later analysis\n",
        "    record[iid] = {\n",
        "        \"image_path\": image_path,\n",
        "        \"question_type\": question_type,\n",
        "        \"question\": question,\n",
        "        \"options\": options,\n",
        "        \"label\": label,\n",
        "        \"model_answer\": model_answer,\n",
        "        \"correct\": model_answer == label\n",
        "    }\n",
        "# ‰øùÂ≠òÁªìÊûú\n",
        "import json\n",
        "with open(\"omnispatial_results.json\", \"w\") as f:\n",
        "    json.dump(record, f, indent=2, ensure_ascii=False)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 43.70%\n"
          ]
        }
      ],
      "source": [
        "result = \"/home/disheng/Spatial_Survey/omnispatial_results.json\"\n",
        "import json\n",
        "record = json.load(open(result, \"r\"))\n",
        "acc = 0\n",
        "total = len(record)\n",
        "correct ={}\n",
        "wrong = {}\n",
        "for id, item in record.items():\n",
        "    label = item[\"label\"]\n",
        "    model_answer = item[\"model_answer\"]\n",
        "    if label in model_answer:\n",
        "        acc += 1\n",
        "        correct[id] = item\n",
        "    else:\n",
        "        wrong[id] = item\n",
        "accuracy = acc / total\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "       \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VSI-Bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'No'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"nyu-visionx/VSI-Bench\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SpatialRGPT-Bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"a8cheng/SpatialRGPT-Bench\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
