{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1117\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:813\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    814\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n",
            "\u001b[0;31mKeyError\u001b[0m: 'qwen2_5_vl'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage-text-to-text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremyxai/SpaceOm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     {\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     },\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     15\u001b[0m pipe(text\u001b[38;5;241m=\u001b[39mmessages)\n",
            "File \u001b[0;32m~/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/pipelines/__init__.py:782\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    779\u001b[0m                 adapter_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    780\u001b[0m                 model \u001b[38;5;241m=\u001b[39m adapter_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_name_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 782\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[1;32m    787\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
            "File \u001b[0;32m~/miniconda3/envs/llava/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1119\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1120\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1121\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1123\u001b[0m         )\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `qwen2_5_vl` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
          ]
        }
      ],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from PIL import Image\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
        "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "pipe(text=messages)\n",
        "\n",
        "def load_and_resize_image(image_path, max_size=1000):\n",
        "    \"\"\"\n",
        "    å®‰å…¨åŠ è½½å›¾ç‰‡ï¼Œè‡ªåŠ¨æ£€æŸ¥ã€é™åˆ†è¾¨ç‡ã€‚\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): å›¾ç‰‡è·¯å¾„\n",
        "        max_size (int): æ¨¡å‹æ”¯æŒçš„æœ€å¤§åˆ†è¾¨ç‡ (shorter edge)\n",
        "    \n",
        "    Returns:\n",
        "        PIL.Image: RGB å›¾ç‰‡ï¼Œå·² resize\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"âŒ Image not found: {image_path}\")\n",
        "\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # å¦‚æœå›¾ç‰‡å¤ªå¤§åˆ™ç­‰æ¯”ç¼©æ”¾\n",
        "    width, height = img.size\n",
        "    if max(width, height) > max_size:\n",
        "        # ç­‰æ¯”ç¼©æ”¾ï¼Œæœ€å¤§è¾¹= max_size\n",
        "        scale = max_size / max(width, height)\n",
        "        new_size = (int(width * scale), int(height * scale))\n",
        "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EgoOrientBench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_base_path = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/\"\n",
        "json_file = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/benchmark.json\"\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "with open(json_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "import os\n",
        "\n",
        "def evaluate_spaceom(pipe, data):\n",
        "    acc = 0\n",
        "    total = len(data)\n",
        "    results = []\n",
        "    for item in tqdm(data):\n",
        "        image_path = f\"{image_base_path}/{item['image']}\"\n",
        "        # img = Image.open(image_path).convert(\"RGB\")\n",
        "        img = load_and_resize_image(image_path, max_size=224)  # Ensure image is resized correctly\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": item[\"question\"]}\n",
        "                ]\n",
        "            },\n",
        "        ]\n",
        "        response = pipe(text=messages)\n",
        "        model_answer = response[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "        results.append({\n",
        "            \"image\": item[\"image\"],\n",
        "            \"question\": item[\"question\"],\n",
        "            \"answer\": model_answer,\n",
        "            \"label\": item[\"label\"],\n",
        "            \"Accuracy\": item[\"label\"] in model_answer\n",
        "        })\n",
        "    #save results to a JSON file\n",
        "    with open(\"EgoOrientBench_spaceom_results.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    print(f\"Accuracy: {sum(1 for r in results if r['Accuracy']) / total * 100:.2f}%\")\n",
        "    print(f\"Total: {total}\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluate_spaceom(pipe, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 33460\n"
          ]
        }
      ],
      "source": [
        "from joblib import Parallel, delayed\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# æ•°æ®è·¯å¾„\n",
        "image_base_path = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/\"\n",
        "json_file = \"/home/disheng/Spatial_Survey/Datasets/EgoOrientBench/benchmark.json\"\n",
        "\n",
        "with open(json_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"Total samples: {len(data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pipe():\n",
        "    from transformers import pipeline\n",
        "    pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\",\n",
        "        device=0  # æ³¨æ„è¿™é‡Œä¸€å®šå†™ device=0ï¼Œå› ä¸ºæ¯ä¸ªè¿›ç¨‹éƒ½åªçœ‹åˆ°è‡ªå·±çš„ GPU\n",
        "    )\n",
        "    return pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected GPUs: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.86it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.66it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.75it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.48it/s]\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "GPU 1:   0%|          | 4/8365 [00:01<44:14,  3.15it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "GPU 0:   0%|          | 10/8365 [00:01<16:19,  8.53it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "GPU 1:   0%|          | 10/8365 [00:02<31:01,  4.49it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "GPU 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8365/8365 [11:11<00:00, 12.45it/s]\n",
            "GPU 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8365/8365 [11:17<00:00, 12.34it/s]\n",
            "GPU 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8365/8365 [16:00<00:00,  8.71it/s]\n",
            "GPU 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 8364/8365 [36:01<00:00,  3.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 12.45%\n",
            "Total: 33460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8365/8365 [36:01<00:00,  3.87it/s]\n"
          ]
        }
      ],
      "source": [
        "def chunkify(lst, n):\n",
        "    \"\"\"æŠŠåˆ—è¡¨å‡åˆ†ä¸º n ä»½\"\"\"\n",
        "    return [lst[i::n] for i in range(n)]\n",
        "\n",
        "def load_and_resize_image(path, max_size=224):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    img.thumbnail((max_size, max_size))\n",
        "    return img\n",
        "\n",
        "\n",
        "def worker(gpu_id, data_chunk):\n",
        "    import os\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "    import torch\n",
        "\n",
        "    # TODO: åœ¨è¿™é‡Œé‡æ–°åŠ è½½ pipe\n",
        "    # ä¾‹å¦‚ï¼š\n",
        "    # from transformers import pipeline\n",
        "    # pipe = pipeline(\"your-task\", device=0)\n",
        "    pipe = load_pipe()  # ä½ éœ€è¦è‡ªå·±å®ç°è¿™ä¸ª\n",
        "\n",
        "    results = []\n",
        "    for item in tqdm(data_chunk, desc=f\"GPU {gpu_id}\"):\n",
        "        image_path = f\"{image_base_path}/{item['image']}\"\n",
        "        img = load_and_resize_image(image_path, max_size=224)\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": item[\"question\"]}\n",
        "                ]\n",
        "            },\n",
        "        ]\n",
        "        response = pipe(text=messages)\n",
        "        model_answer = response[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "        results.append({\n",
        "            \"image\": item[\"image\"],\n",
        "            \"question\": item[\"question\"],\n",
        "            \"answer\": model_answer,\n",
        "            \"label\": item[\"label\"],\n",
        "            \"Accuracy\": item[\"label\"] in model_answer\n",
        "        })\n",
        "    return results\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"Detected GPUs: {num_gpus}\")\n",
        "\n",
        "# å‡åŒ€åˆ†å—\n",
        "chunks = chunkify(data, num_gpus)\n",
        "\n",
        "# å¹¶è¡Œæ‰§è¡Œ\n",
        "all_results = Parallel(n_jobs=num_gpus)(\n",
        "    delayed(worker)(gpu_id, chunk) for gpu_id, chunk in enumerate(chunks)\n",
        ")\n",
        "\n",
        "# å±•å¹³\n",
        "flat_results = [item for sublist in all_results for item in sublist]\n",
        "\n",
        "# ä¿å­˜\n",
        "with open(\"EgoOrientBench_spaceom_results.json\", \"w\") as f:\n",
        "    json.dump(flat_results, f, indent=4)\n",
        "\n",
        "acc = sum(1 for r in flat_results if r['Accuracy']) / len(flat_results)\n",
        "print(f\"Accuracy: {acc * 100:.2f}%\")\n",
        "print(f\"Total: {len(flat_results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 40.82%\n"
          ]
        }
      ],
      "source": [
        "path = \"/home/disheng/Spatial_Survey/Spatial_VLM_Survey/code/evaluation/EgoOrientBench_spaceom_results.json\"\n",
        "data = json.load(open(path, \"r\"))\n",
        "acc  = 0\n",
        "for item in data:\n",
        "    answer = item[\"answer\"].lower()\n",
        "    label = item[\"label\"].lower()\n",
        "    if label in answer:\n",
        "        acc += 1\n",
        "print(f\"Accuracy: {acc / len(data) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GeoMeter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### real data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 16.25it/s]\n",
            "Device set to use cuda:2\n",
            "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:18<00:00,  5.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â–¶ï¸ Overall Accuracy: 43.00%\n",
            "number of accurate samples: 43\n",
            "number of samples: 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# ğŸš€ 1) ç¯å¢ƒå‡†å¤‡\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# %%\n",
        "# âš™ï¸ 2) å‚æ•°é…ç½®\n",
        "IMAGE_BASE   = Path(\"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Real/\")\n",
        "JSONL_FILE   = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Real/depth_height_1000_realworld.jsonl\"\n",
        "MODEL_NAME   = \"remyxai/SpaceOm\"\n",
        "TASK         = \"image-text-to-text\"\n",
        "DEVICE_ID    = 2      # å•å¡å°±ç”¨ 0\n",
        "BATCH_SIZE   = 1     # æ ¹æ®æ˜¾å­˜è°ƒ\n",
        "\n",
        "# %%\n",
        "# ğŸ“– 3) è¯»æ•°æ®\n",
        "data = []\n",
        "with open(JSONL_FILE, \"r\") as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        assert \"images\" in item and \"query_text\" in item and \"target_text\" in item\n",
        "        data.append(item)\n",
        "\n",
        "# %%\n",
        "# ğŸ”§ 4) åˆå§‹åŒ– pipeline\n",
        "pipe = pipeline(\n",
        "    TASK,\n",
        "    model=MODEL_NAME,\n",
        "    device=DEVICE_ID,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# %%\n",
        "# ğŸƒ 5) æ‰¹é‡æ¨ç† + è¿›åº¦æ¡\n",
        "results = []\n",
        "for i in tqdm(range(0, len(data), BATCH_SIZE), desc=\"Inference\"):\n",
        "    batch = data[i : i + BATCH_SIZE]\n",
        "    messages = []\n",
        "    for item in batch:\n",
        "        img = Image.open(IMAGE_BASE / item[\"images\"][0]).convert(\"RGB\")\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img},\n",
        "                {\"type\": \"text\",  \"text\":  item[\"query_text\"]}\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    outputs = pipe(text=messages)\n",
        "\n",
        "    for item, out in zip(batch, outputs):\n",
        "        # å– generated_text è¿™ä¸ª listï¼Œç„¶åæ‰¾å‡º assistant é‚£æ¡\n",
        "        gen_list = out.get(\"generated_text\", [])\n",
        "        assistant_entry = next(\n",
        "            (entry for entry in gen_list if entry.get(\"role\")==\"assistant\"),\n",
        "            None\n",
        "        )\n",
        "        if assistant_entry is None:\n",
        "            # ä¸‡ä¸€æ²¡æ‰¾åˆ°ï¼Œå°±é™çº§å¤„ç†\n",
        "            raw_text = str(gen_list)\n",
        "        else:\n",
        "            raw_text = assistant_entry.get(\"content\", \"\")\n",
        "\n",
        "        # ç”¨æ­£åˆ™å»æ‰æœ«å°¾å¤šä½™çš„é€—å·ã€å¥å·\n",
        "        pred = re.sub(r\"[ï¼Œ,\\.ã€‚]+$\", \"\", raw_text).strip()\n",
        "\n",
        "        results.append({\n",
        "            \"image\": item[\"images\"][0],\n",
        "            \"query\": item[\"query_text\"],\n",
        "            \"pred\":  pred,\n",
        "            \"gold\":  item[\"target_text\"]\n",
        "        })\n",
        "\n",
        "# %%\n",
        "# ğŸ“Š 6) è®¡ç®—å‡†ç¡®ç‡ & ä¿å­˜\n",
        "accuracy = 0.0\n",
        "df = pd.DataFrame(results)\n",
        "for pred, gold in zip(df[\"pred\"], df[\"gold\"]):\n",
        "    if pred.lower() in gold.lower():\n",
        "        accuracy += 1\n",
        "accuracy /= len(df)\n",
        "print(f\"â–¶ï¸ Overall Accuracy: {accuracy:.2%}\")\n",
        "print(\"number of accurate samples:\", int(accuracy * len(df)))\n",
        "print(\"number of samples:\", len(df))\n",
        "\n",
        "# df.to_json(\"geobench_real_results_with_predictions.json\",\n",
        "#            orient=\"records\", indent=2, force_ascii=False)\n",
        "# print(\"âœ… ç»“æœå·²ä¿å­˜åˆ° geobench_real_results_with_predictions.{json,csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### synthetic data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# depth\n",
        "\n",
        "base_ir = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/depth\"\n",
        "dir_list = os.listdir(base_ir)\n",
        "\n",
        "depth_results = {}\n",
        "depth_3_shapes = {}\n",
        "depth_5_shapes = {}\n",
        "for each_sub_set in dir_list: # [images-3-shapes, images-5-shapes]\n",
        "    each_shape_set_results = {}\n",
        "    \n",
        "    for each_example in os.listdir(os.path.join(base_ir, each_sub_set, \"prompts\")):  #each prompt\n",
        "        prompts_json = os.path.join(base_ir, each_sub_set, \"prompts\", each_example)\n",
        "        with open(prompts_json, \"r\") as f:\n",
        "            prompts = json.load(f)\n",
        "\n",
        "        image_name = prompts[\"filename\"]\n",
        "        image_path = os.path.join(base_ir, each_sub_set, \"imgs\", image_name)\n",
        "        image_labelled_path = os.path.join(base_ir, each_sub_set, \"labelled\", image_name)\n",
        "        image_labelled_id_path = os.path.join(base_ir, each_sub_set, \"labelled_id\", image_name)\n",
        "        image_labelled_id_reverse_path = os.path.join(base_ir, each_sub_set, \"labelled_id_reverse\", image_name)\n",
        "\n",
        "        prompts_plain = prompts[\"prompts\"]\n",
        "        prompts_labelled = prompts[\"prompts_labelled\"]\n",
        "        prompts_labelled_id = prompts[\"prompts_labelled_id\"]\n",
        "\n",
        "        plain_results = {}\n",
        "        labelled_results = {}\n",
        "        labelled_id_results = {}\n",
        "        labelled_id_reverse_results = {}\n",
        "\n",
        "        for index, each_prompt in enumerate(prompts_plain):\n",
        "            answer = each_prompt[\"answer\"]\n",
        "            textual_prompt = each_prompt[\"prompt\"] \n",
        "            answer_set = \";\\n\".join(each_prompt[\"answerSet\"])\n",
        "\n",
        "            full_prompt = textual_prompt + \"\\nAnswer Set:\\n\" + answer_set\n",
        "\n",
        "            messages = []\n",
        "            img = load_and_resize_image(image_path)\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\",  \"text\":  full_prompt}\n",
        "                ]\n",
        "            })\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "\n",
        "            plain_results[index] = {\n",
        "                \"image\": image_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "        \n",
        "        for index, each_prompt in enumerate(prompts_labelled):\n",
        "            answer = each_prompt[\"answer\"]\n",
        "            textual_prompt = each_prompt[\"prompt\"] \n",
        "            answer_set = \";\\n\".join(each_prompt[\"answerSet\"])\n",
        "\n",
        "            full_prompt = textual_prompt + \"\\nAnswer Set:\\n\" + answer_set\n",
        "\n",
        "            messages = []\n",
        "            img = load_and_resize_image(image_labelled_path)\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\",  \"text\":  full_prompt}\n",
        "                ]\n",
        "            })\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"] \n",
        "\n",
        "            labelled_results[index] = {\n",
        "                \"image\": image_labelled_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "\n",
        "        for index, each_prompt in enumerate(prompts_labelled_id):\n",
        "            answer = each_prompt[\"answer\"]\n",
        "            textual_prompt = each_prompt[\"prompt\"] \n",
        "            answer_set = \";\\n\".join(each_prompt[\"answerSet\"])\n",
        "\n",
        "            full_prompt = textual_prompt + \"\\nAnswer Set:\\n\" + answer_set\n",
        "\n",
        "            messages = []\n",
        "            img = load_and_resize_image(image_labelled_id_path)\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\",  \"text\":  full_prompt}\n",
        "                ]\n",
        "            })\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]     \n",
        "            labelled_id_results[index] = {\n",
        "                \"image\": image_labelled_id_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "\n",
        "        for index, each_prompt in enumerate(prompts_labelled_id):\n",
        "            answer = each_prompt[\"answerReverse\"]\n",
        "            textual_prompt = each_prompt[\"prompt\"] \n",
        "            answer_set = \";\\n\".join(each_prompt[\"answerSet\"])\n",
        "\n",
        "            full_prompt = textual_prompt + \"\\nAnswer Set:\\n\" + answer_set\n",
        "\n",
        "            messages = []\n",
        "            img = load_and_resize_image(image_labelled_id_reverse_path)\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\",  \"text\":  full_prompt}\n",
        "                ]\n",
        "            })\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "            labelled_id_reverse_results[index] = {\n",
        "                \"image\": image_labelled_id_reverse_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "        if \"3-shapes\" in each_sub_set:\n",
        "            depth_3_shapes[each_sub_set] = {\n",
        "                \"plain\": plain_results,\n",
        "                \"labelled\": labelled_results,\n",
        "                \"labelled_id\": labelled_id_results,\n",
        "                \"labelled_id_reverse\": labelled_id_reverse_results\n",
        "            }\n",
        "        elif \"5-shapes\" in each_sub_set:\n",
        "            depth_5_shapes[each_sub_set] = {\n",
        "                \"plain\": plain_results,\n",
        "                \"labelled\": labelled_results,\n",
        "                \"labelled_id\": labelled_id_results,\n",
        "                \"labelled_id_reverse\": labelled_id_reverse_results\n",
        "            }   \n",
        "\n",
        "    depth_results[each_sub_set] = {\n",
        "        \"plain\": plain_results, \n",
        "        \"labelled\": labelled_results,\n",
        "        \"labelled_id\": labelled_id_results,\n",
        "        \"labelled_id_reverse\": labelled_id_reverse_results\n",
        "    }\n",
        "\n",
        "# Save results          \n",
        "import json\n",
        "depth_results_path = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/depth/depth_results.json\"\n",
        "with open(depth_results_path, \"w\") as f:\n",
        "    json.dump({\n",
        "        \"depth_3_shapes\": depth_3_shapes,\n",
        "        \"depth_5_shapes\": depth_5_shapes,\n",
        "        \"depth_results\": depth_results\n",
        "    }, f, indent=4)     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_resize_image(path, max_size=224):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    img.thumbnail((max_size, max_size))\n",
        "    return img\n",
        "\n",
        "def chunkify(lst, n):\n",
        "    return [lst[i::n] for i in range(n)]\n",
        "\n",
        "def load_pipe():\n",
        "    from transformers import pipeline\n",
        "    pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\",\n",
        "    device=0  # æ³¨æ„è¿™é‡Œä¸€å®šå†™ device=0ï¼Œå› ä¸ºæ¯ä¸ªè¿›ç¨‹éƒ½åªçœ‹åˆ°è‡ªå·±çš„ GPU\n",
        "    )\n",
        "    return pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 4 GPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.59it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.88it/s]\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Subset images-5-shapes:   0%|          | 0/400 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Subset images-3-shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:03<00:00,  5.43s/it] \n",
            "Subset images-5-shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [1:01:12<00:00,  9.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to: /home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/depth/depth_results.json\n"
          ]
        }
      ],
      "source": [
        "def process_subset(subset_name):\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    pipe = load_pipe()  # åªåœ¨è¿™é‡ŒåŠ è½½ï¼Œæ¯ä¸ª GPU ä¸€ä»½\n",
        "\n",
        "    each_shape_set_results = {}\n",
        "\n",
        "    prompts_dir = os.path.join(base_ir, subset_name, \"prompts\")\n",
        "    examples = os.listdir(prompts_dir)\n",
        "\n",
        "    for each_example in tqdm(examples, desc=f\"Subset {subset_name}\"):\n",
        "        with open(os.path.join(prompts_dir, each_example), \"r\") as f:\n",
        "            prompts = json.load(f)\n",
        "\n",
        "        img_base = os.path.join(base_ir, subset_name)\n",
        "        paths = {\n",
        "            \"plain\": os.path.join(img_base, \"imgs\", prompts[\"filename\"]),\n",
        "            \"labelled\": os.path.join(img_base, \"labelled\", prompts[\"filename_labelled\"]),\n",
        "            \"labelled_id\": os.path.join(img_base, \"labelled_id\", prompts[\"filename_labelled\"]),\n",
        "            \"labelled_reverse_id\": os.path.join(img_base, \"labelled_reverse_id\", prompts[\"filename_labelled\"])\n",
        "        }\n",
        "\n",
        "        def run_prompts(prompts_list, img_path, is_reverse=False):\n",
        "            output = {}\n",
        "            for index, p in enumerate(prompts_list):\n",
        "                answer = p[\"answerReverse\"] if is_reverse else p[\"answer\"]\n",
        "                full_prompt = p[\"prompt\"] + \"\\nAnswer Set:\\n\" + \";\\n\".join(p[\"answerSet\"])\n",
        "                img = load_and_resize_image(img_path)\n",
        "\n",
        "                messages = [{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"image\", \"image\": img},\n",
        "                        {\"type\": \"text\", \"text\": full_prompt}\n",
        "                    ]\n",
        "                }]\n",
        "                try:\n",
        "                    inference_result = pipe(text=messages)\n",
        "                    final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "                    final_answer = \"ERROR\"\n",
        "\n",
        "                output[index] = {\n",
        "                    \"image\": img_path,\n",
        "                    \"prompt\": full_prompt,\n",
        "                    \"answer\": answer,\n",
        "                    \"pred\": final_answer,\n",
        "                    \"accuracy\": answer.lower() in final_answer.lower()\n",
        "                }\n",
        "            return output\n",
        "\n",
        "        plain = run_prompts(prompts[\"prompts\"], paths[\"plain\"])\n",
        "        labelled = run_prompts(prompts[\"prompts_labelled\"], paths[\"labelled\"])\n",
        "        labelled_id = run_prompts(prompts[\"prompts_labelled_id\"], paths[\"labelled_id\"])\n",
        "        labelled_reverse_id = run_prompts(prompts[\"prompts_labelled_id\"], paths[\"labelled_reverse_id\"], is_reverse=True)\n",
        "\n",
        "        each_shape_set_results[each_example] = {\n",
        "            \"plain\": plain,\n",
        "            \"labelled\": labelled,\n",
        "            \"labelled_id\": labelled_id,\n",
        "            \"labelled_reverse_id\": labelled_reverse_id\n",
        "        }\n",
        "\n",
        "    return (subset_name, each_shape_set_results)\n",
        "\n",
        "\n",
        "def main_parallel():\n",
        "    num_gpus = 4  # ä½ æœ‰ 4 å¼  GPU\n",
        "    print(f\"Using {num_gpus} GPUs\")\n",
        "\n",
        "    chunks = chunkify(dir_list, num_gpus)\n",
        "\n",
        "    def worker(gpu_id, chunk):\n",
        "        import os\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "        results = {}\n",
        "        for subset in chunk:\n",
        "            name, res = process_subset(subset)\n",
        "            results[name] = res\n",
        "        return results\n",
        "\n",
        "    from joblib import Parallel, delayed\n",
        "\n",
        "    all_results = Parallel(n_jobs=num_gpus)(\n",
        "        delayed(worker)(gpu_id, chunk) for gpu_id, chunk in enumerate(chunks)\n",
        "    )\n",
        "\n",
        "    # åˆå¹¶\n",
        "    depth_results = {}\n",
        "    for part in all_results:\n",
        "        depth_results.update(part)\n",
        "\n",
        "    depth_3_shapes = {k: v for k, v in depth_results.items() if \"3-shapes\" in k}\n",
        "    depth_5_shapes = {k: v for k, v in depth_results.items() if \"5-shapes\" in k}\n",
        "\n",
        "    save_path = os.path.join(base_ir, \"depth_results.json\")\n",
        "    with open(save_path, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"depth_3_shapes\": depth_3_shapes,\n",
        "            \"depth_5_shapes\": depth_5_shapes,\n",
        "            \"depth_results\": depth_results\n",
        "        }, f, indent=4)\n",
        "\n",
        "    print(f\"Saved to: {save_path}\")\n",
        "\n",
        "main_parallel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 34.85%\n",
            "Accuracy: 28.84%\n",
            "number of Accurate:  4811\n",
            "number of Total:  16684\n"
          ]
        }
      ],
      "source": [
        "path = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/depth/depth_results.json\"\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "depth_results = json.load(open(path, \"r\"))\n",
        "depth_3_shapes = depth_results[\"depth_3_shapes\"]\n",
        "depth_5_shapes = depth_results[\"depth_5_shapes\"]\n",
        "depth_results = depth_results[\"depth_results\"]\n",
        "acc = 0\n",
        "total = 0\n",
        "for json in depth_3_shapes[\"images-3-shapes\"]:\n",
        "    plain = depth_3_shapes[\"images-3-shapes\"][json][\"plain\"]\n",
        "    labelled = depth_3_shapes[\"images-3-shapes\"][json][\"labelled\"]\n",
        "    labelled_id = depth_3_shapes[\"images-3-shapes\"][json][\"labelled_id\"]\n",
        "    labelled_reverse_id = depth_3_shapes[\"images-3-shapes\"][json][\"labelled_reverse_id\"]\n",
        "    acc += sum(1 for v in plain.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled_id.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled_reverse_id.values() if v[\"accuracy\"])\n",
        "    total += len(plain) + len(labelled) + len(labelled_id) + len(labelled_reverse_id)\n",
        "print(f\"Accuracy: {acc / total * 100:.2f}%\")\n",
        "\n",
        "for json in depth_5_shapes[\"images-5-shapes\"]:\n",
        "    plain = depth_5_shapes[\"images-5-shapes\"][json][\"plain\"]\n",
        "    labelled = depth_5_shapes[\"images-5-shapes\"][json][\"labelled\"]\n",
        "    labelled_id = depth_5_shapes[\"images-5-shapes\"][json][\"labelled_id\"]\n",
        "    labelled_reverse_id = depth_5_shapes[\"images-5-shapes\"][json][\"labelled_reverse_id\"]\n",
        "    acc += sum(1 for v in plain.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled_id.values() if v[\"accuracy\"])\n",
        "    acc += sum(1 for v in labelled_reverse_id.values() if v[\"accuracy\"])\n",
        "    total += len(plain) + len(labelled) + len(labelled_id) + len(labelled_reverse_id)\n",
        "print(f\"Accuracy: {acc / total * 100:.2f}%\")\n",
        "print(\"number of Accurate: \", acc)\n",
        "print(\"number of Total: \", total)\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### heigh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "base_path = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/height\"\n",
        "sub_classes = os.listdir(base_path)\n",
        "\n",
        "height_results = {}\n",
        "\n",
        "print(f\"Found {len(sub_classes)} sub-classes: {sub_classes}\")\n",
        "\n",
        "for each_sub_class in sub_classes:\n",
        "    print(f\"Processing {each_sub_class}...\")\n",
        "    sub_class_path = os.path.join(base_path, each_sub_class)\n",
        "    prompts_dir = os.path.join(sub_class_path, \"prompts\")\n",
        "    prompts_files = os.listdir(prompts_dir)\n",
        "\n",
        "    for each_prompt_file in tqdm(prompts_files, desc=f\"{each_sub_class} prompts\"):\n",
        "        prompt_path = os.path.join(prompts_dir, each_prompt_file)\n",
        "        with open(prompt_path, \"r\") as f:\n",
        "            prompt_json = json.load(f)\n",
        "\n",
        "        image_name = prompt_json[\"filename\"]\n",
        "        image_path = os.path.join(sub_class_path, \"imgs\", image_name)\n",
        "        prompts_list = prompt_json[\"prompts\"]\n",
        "\n",
        "        for index, prompt in enumerate(prompts_list):\n",
        "            answer = prompt[\"answer\"]\n",
        "            answer_set = \";\\n\".join(prompt[\"answerSet\"])\n",
        "            full_prompt = prompt[\"prompt\"] + \"\\nAnswer Set:[\\n\" + answer_set + \"\\n]\"\n",
        "\n",
        "            img = load_and_resize_image(image_path)\n",
        "            messages = [{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": full_prompt}\n",
        "                ]\n",
        "            }]\n",
        "\n",
        "            inference_result = pipe(text=messages)\n",
        "            final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "\n",
        "            # è®°å½•ç»“æœï¼Œåˆ†å±‚ç»“æ„: {sub_class: {image_name: {index: ...}}}\n",
        "            height_results.setdefault(each_sub_class, {})\n",
        "            height_results[each_sub_class].setdefault(image_name, {})\n",
        "            height_results[each_sub_class][image_name][index] = {\n",
        "                \"image\": image_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "\n",
        "# ä¿å­˜ç»“æœ\n",
        "height_results_path = os.path.join(base_path, \"height_results.json\")\n",
        "with open(height_results_path, \"w\") as f:\n",
        "    json.dump(height_results, f, indent=4)\n",
        "\n",
        "print(f\"Results saved to: {height_results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8 sub-classes: ['images-3-stacks-colored', 'images-3-stacks-stepped-colored', 'images-3-stacks-stepped', 'images-5-stacks-stepped', 'images-5-stacks-colored', 'images-3-stacks', 'images-5-stacks', 'images-5-stacks-stepped-colored']\n",
            "Detected GPUs: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.36it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.82it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.41it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.71it/s]\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Sub-class: images-3-stacks-stepped:   2%|â–         | 4/200 [00:01<01:09,  2.81it/s].60it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Sub-class: images-5-stacks-stepped:   1%|          | 2/200 [00:01<02:41,  1.22it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Sub-class: images-3-stacks-stepped-colored: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [01:02<00:00,  4.01it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 18.09it/s]\n",
            "Sub-class: images-5-stacks-stepped:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [01:03<01:09,  1.49it/s]Device set to use cuda:0\n",
            "Sub-class: images-3-stacks-stepped: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:22<00:00,  2.43it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.18it/s]0,  1.47it/s]\n",
            "Sub-class: images-3-stacks:  22%|â–ˆâ–ˆâ–       | 45/200 [00:20<01:30,  1.72it/s]Device set to use cuda:0\n",
            "Sub-class: images-5-stacks-stepped: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:11<00:00,  1.52it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.44it/s]t/s]\n",
            "Sub-class: images-3-stacks-colored:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 238/270 [02:12<00:18,  1.76it/s]Device set to use cuda:0\n",
            "Sub-class: images-3-stacks-colored: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [02:30<00:00,  1.79it/s]38it/s]\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 17.52it/s]it/s]:54,  1.30it/s]\n",
            "Sub-class: images-5-stacks-stepped-colored:  10%|â–ˆ         | 25/250 [00:17<02:38,  1.42it/s]Device set to use cuda:0\n",
            "Sub-class: images-3-stacks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:31<00:00,  2.17it/s]:28,  1.47it/s]\n",
            "Sub-class: images-5-stacks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [02:24<00:00,  1.39it/s].30it/s].30it/s]\n",
            "Sub-class: images-5-stacks-stepped-colored: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:00<00:00,  1.38it/s]\n",
            "Sub-class: images-5-stacks-colored: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 249/250 [02:59<00:00,  1.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: /home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/height/height_results.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sub-class: images-5-stacks-colored: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:00<00:00,  1.39it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import torch\n",
        "\n",
        "# è·¯å¾„\n",
        "base_path = \"/home/disheng/Spatial_Survey/Datasets/GeoMeter/Synthetic/height\"\n",
        "sub_classes = os.listdir(base_path)\n",
        "print(f\"Found {len(sub_classes)} sub-classes: {sub_classes}\")\n",
        "\n",
        "def chunkify(lst, n):\n",
        "    \"\"\"å°†åˆ—è¡¨å‡åŒ€åˆ†å—\"\"\"\n",
        "    return [lst[i::n] for i in range(n)]\n",
        "\n",
        "def process_height_sub_class(sub_class):\n",
        "    from tqdm.auto import tqdm\n",
        "    import os\n",
        "\n",
        "    pipe = load_pipe()  # æ¯å¼  GPU ç‹¬ç«‹åŠ è½½\n",
        "\n",
        "    sub_class_path = os.path.join(base_path, sub_class)\n",
        "    prompts_dir = os.path.join(sub_class_path, \"prompts\")\n",
        "    prompt_files = os.listdir(prompts_dir)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for each_prompt_file in tqdm(prompt_files, desc=f\"Sub-class: {sub_class}\"):\n",
        "        with open(os.path.join(prompts_dir, each_prompt_file), \"r\") as f:\n",
        "            prompt_json = json.load(f)\n",
        "\n",
        "        image_name = prompt_json[\"filename\"]\n",
        "        image_path = os.path.join(sub_class_path, \"imgs\", image_name)\n",
        "        prompts_list = prompt_json[\"prompts\"]\n",
        "\n",
        "        for index, prompt in enumerate(prompts_list):\n",
        "            answer = prompt[\"answer\"]\n",
        "            answer_set = \";\\n\".join(prompt[\"answerSet\"])\n",
        "            full_prompt = prompt[\"prompt\"] + \"\\nAnswer Set:[\\n\" + answer_set + \"\\n]\\nJust select the answer from the set without any explanation.\"\n",
        "\n",
        "            img = load_and_resize_image(image_path)\n",
        "            messages = [{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": img},\n",
        "                    {\"type\": \"text\", \"text\": full_prompt}\n",
        "                ]\n",
        "            }]\n",
        "\n",
        "            try:\n",
        "                inference_result = pipe(text=messages)\n",
        "                final_answer = inference_result[0].get(\"generated_text\", [])[-1][\"content\"]\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                final_answer = \"ERROR\"\n",
        "\n",
        "            # ç»“æ„: {sub_class: {image_name: {index: ...}}}\n",
        "            results.setdefault(sub_class, {})\n",
        "            results[sub_class].setdefault(image_name, {})\n",
        "            results[sub_class][image_name][index] = {\n",
        "                \"image\": image_path,\n",
        "                \"prompt\": full_prompt,\n",
        "                \"answer\": answer,\n",
        "                \"pred\": final_answer,\n",
        "                \"accuracy\": answer.lower() in final_answer.lower(),\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "def worker(gpu_id, chunk):\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "    gpu_results = {}\n",
        "\n",
        "    for sub_class in chunk:\n",
        "        res = process_height_sub_class(sub_class)\n",
        "        gpu_results.update(res)\n",
        "\n",
        "    return gpu_results\n",
        "\n",
        "def run_height_parallel():\n",
        "    num_gpus = torch.cuda.device_count()\n",
        "    print(f\"Detected GPUs: {num_gpus}\")\n",
        "\n",
        "    chunks = chunkify(sub_classes, num_gpus)\n",
        "\n",
        "    all_results = Parallel(n_jobs=num_gpus)(\n",
        "        delayed(worker)(gpu_id, chunk) for gpu_id, chunk in enumerate(chunks)\n",
        "    )\n",
        "\n",
        "    # åˆå¹¶\n",
        "    height_results = {}\n",
        "    for part in all_results:\n",
        "        height_results.update(part)\n",
        "\n",
        "    # ä¿å­˜\n",
        "    save_path = os.path.join(base_path, \"height_results.json\")\n",
        "    with open(save_path, \"w\") as f:\n",
        "        json.dump(height_results, f, indent=4)\n",
        "\n",
        "    print(f\"Results saved to: {save_path}\")\n",
        "\n",
        "\n",
        "run_height_parallel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 24.23%\n",
            "number of accurate results: 1713\n",
            "total number of results: 7069\n"
          ]
        }
      ],
      "source": [
        "acc = 0\n",
        "total = 0\n",
        "height_results_path = os.path.join(base_path, \"height_results.json\")\n",
        "height_results = json.load(open(height_results_path, \"r\"))\n",
        "for sub_class, images in height_results.items():\n",
        "    for image_name, prompts in images.items():\n",
        "        for index, result in prompts.items():\n",
        "            if result[\"accuracy\"]:\n",
        "                acc += 1\n",
        "            total += 1\n",
        "print(f\"Accuracy: {acc / total * 100:.2f}%\")\n",
        "print(\"number of accurate results:\", acc)\n",
        "print(\"total number of results:\", total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final Accuracy:  27.53112815997988\n"
          ]
        }
      ],
      "source": [
        "number_of_samples = 7069 + 100 + 16684\n",
        "number_of_accurate_samples = 1713 + 4811 + 43\n",
        "print(\"final Accuracy: \", number_of_accurate_samples / number_of_samples * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OmniSpatial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# export HF_ENDPOINT=\"https://hf-mirror.com\"\n",
        "!mkdir -p dataset\n",
        "!huggingface-cli download --resume-download qizekun/OmniSpatial --local-dir dataset --repo-type dataset\n",
        "!find dataset/ -name '*.zip' -exec unzip -o {} -d dataset/ \\;\n",
        "!rm -f dataset/*.zip && rm -rf dataset/__MACOSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!export CUDA_VISIBLE_DEVICES=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def load_and_resize_image(image_path, max_size=448):\n",
        "    \"\"\"\n",
        "    å®‰å…¨åŠ è½½å›¾ç‰‡ï¼Œè‡ªåŠ¨æ£€æŸ¥ã€é™åˆ†è¾¨ç‡ã€‚\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): å›¾ç‰‡è·¯å¾„\n",
        "        max_size (int): æ¨¡å‹æ”¯æŒçš„æœ€å¤§åˆ†è¾¨ç‡ (shorter edge)\n",
        "    \n",
        "    Returns:\n",
        "        PIL.Image: RGB å›¾ç‰‡ï¼Œå·² resize\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"âŒ Image not found: {image_path}\")\n",
        "\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # å¦‚æœå›¾ç‰‡å¤ªå¤§åˆ™ç­‰æ¯”ç¼©æ”¾\n",
        "    width, height = img.size\n",
        "    if max(width, height) > max_size:\n",
        "        # ç­‰æ¯”ç¼©æ”¾ï¼Œæœ€å¤§è¾¹= max_size\n",
        "        scale = max_size / max(width, height)\n",
        "        new_size = (int(width * scale), int(height * scale))\n",
        "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 13.64it/s]\n",
            "Device set to use cuda:0\n",
            "  1%|          | 10/1533 [00:01<03:29,  7.27it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 551/1533 [01:47<05:12,  3.14it/s]/home/disheng/miniconda3/lib/python3.12/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1533/1533 [04:14<00:00,  6.02it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from PIL import Image\n",
        "# pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "question_types =  {0:\"Dynamic_Reasoning\", \n",
        "                   1:\"Spatial_Interaction\", \n",
        "                   2:\"Complex_Logic\", \n",
        "                   3:\"Perspective_Taking\"}\n",
        "annotation = \"/home/disheng/Spatial_Survey/Datasets/OmniSpatial/dataset/data.json\"\n",
        "import json\n",
        "annotation_data = json.load(open(annotation, \"r\"))\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "record = {}\n",
        "for item in tqdm(annotation_data):\n",
        "    iid = item[\"id\"]\n",
        "    image_id = item[\"id\"].split(\"_\")[0]\n",
        "    question_type = item[\"task_type\"]\n",
        "\n",
        "    question = item[\"question\"]\n",
        "    options = item[\"options\"]\n",
        "    full_prompt = f\"Question: {question}\\nOptions: {\"; \".join(options)}. Please only retuen a correct option without analysis.\"\n",
        "    label = item[\"answer\"]\n",
        "    if len(options) != 0:\n",
        "        label = options[label]\n",
        "    image_path = f\"/home/disheng/Spatial_Survey/Datasets/OmniSpatial/dataset/{question_type}/{image_id}.png\"\n",
        "    img = load_and_resize_image(image_path, max_size=448)  # ä½¿ç”¨å®‰å…¨åŠ è½½å‡½æ•°\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img},\n",
        "                {\"type\": \"text\", \"text\": full_prompt}\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    try:\n",
        "        response = pipe(text=messages)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {iid}: {e}\")\n",
        "        print(image_path)\n",
        "        print(options)\n",
        "        print(full_prompt)\n",
        "        print(label)\n",
        "        print(messages)\n",
        "        print()\n",
        "        continue\n",
        "    model_answer = response[0][\"generated_text\"][-1][\"content\"]\n",
        "    # record all of information for later analysis\n",
        "    record[iid] = {\n",
        "        \"image_path\": image_path,\n",
        "        \"question_type\": question_type,\n",
        "        \"question\": question,\n",
        "        \"options\": options,\n",
        "        \"label\": label,\n",
        "        \"model_answer\": model_answer,\n",
        "        \"correct\": model_answer == label\n",
        "    }\n",
        "# ä¿å­˜ç»“æœ\n",
        "import json\n",
        "with open(\"omnispatial_results.json\", \"w\") as f:\n",
        "    json.dump(record, f, indent=2, ensure_ascii=False)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 43.70%\n"
          ]
        }
      ],
      "source": [
        "result = \"/home/disheng/Spatial_Survey/omnispatial_results.json\"\n",
        "import json\n",
        "record = json.load(open(result, \"r\"))\n",
        "acc = 0\n",
        "total = len(record)\n",
        "correct ={}\n",
        "wrong = {}\n",
        "for id, item in record.items():\n",
        "    label = item[\"label\"]\n",
        "    model_answer = item[\"model_answer\"]\n",
        "    if label in model_answer:\n",
        "        acc += 1\n",
        "        correct[id] = item\n",
        "    else:\n",
        "        wrong[id] = item\n",
        "accuracy = acc / total\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "       \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MM-Vet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "# Load a specific split \n",
        "dataset = load_dataset(\"LLDDSS/Awesome_Spatial_VLMs\", split=\"mm_vet\")\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14507d3c3847415e94d17de82c595e79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a31ab6d59284f1f8cc1b89af2ad95d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/75 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 75\n",
            "Correct records: 36\n",
            "Accuracy: 48.00%\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.with_format(\"torch\", columns=['id', 'image', 'question', 'options', 'GT'])\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_loader = DataLoader(dataset,)\n",
        "records = {}\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "\n",
        "def check_correctness(model_answer, GT):\n",
        "    GT = GT[0]\n",
        "    if \"<AND>\" in GT:\n",
        "        # å¤„ç† AND é€»è¾‘\n",
        "        gt_answers = GT.split(\"<AND>\")\n",
        "        return all(answer.strip().lower() in model_answer.lower() for answer in gt_answers)\n",
        "    elif \"<OR>\" in GT:\n",
        "        # å¤„ç† OR é€»è¾‘\n",
        "        gt_answers = GT.split(\"<OR>\")\n",
        "        return any(answer.strip().lower() in model_answer.lower() for answer in gt_answers)\n",
        "    return GT.strip().lower() in model_answer.lower()\n",
        "\n",
        "for batch in tqdm(data_loader):\n",
        "    id = batch['id'][0]\n",
        "    img = batch['image']  # shape: (B, C, H, W)\n",
        "\n",
        "    # print(\"Type:\", type(imgs))\n",
        "    # print(\"Shape:\", imgs.shape)\n",
        "\n",
        "    # å¯è§†åŒ–ç¬¬ 0 å¼ å›¾åƒ\n",
        "    img_pil = to_pil_image(img[0])  # shape: [4, H, W]\n",
        "\n",
        "\n",
        "    full_prompt = batch['question']   \n",
        "    GT = batch['GT']\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img_pil},\n",
        "                {\"type\": \"text\", \"text\": full_prompt}\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    try:\n",
        "        response = pipe(text=messages)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {id}: {e}\")\n",
        "        raise e\n",
        "    model_answer = response[0][\"generated_text\"][-1][\"content\"]\n",
        "    whether_correct = check_correctness(model_answer, GT)\n",
        "    # record all of information for later analysis\n",
        "    records[id] = {\n",
        "        \"question\": full_prompt,\n",
        "        \"GT\": GT,\n",
        "        \"model_answer\": model_answer,\n",
        "        \"correct\": whether_correct\n",
        "    }\n",
        "# ä¿å­˜ç»“æœ\n",
        "print(f\"Total records: {len(records)}\")\n",
        "print(f\"Correct records: {sum(1 for r in records.values() if r['correct'])}\")\n",
        "print(f\"Accuracy: {sum(1 for r in records.values() if r['correct']) / len(records):.2%}\")\n",
        "import json\n",
        "with open(\"mm_vet.json\", \"w\") as f:\n",
        "    json.dump(records, f, indent=2, ensure_ascii=False)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## what is up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "# Load a specific split \n",
        "dataset = load_dataset(\"LLDDSS/Awesome_Spatial_VLMs\", split=\"whats_up\")\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'image', 'question', 'options', 'GT'],\n",
              "    num_rows: 820\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "820feed429424e4595a06cd5e2e7f788",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6283ba00e5942cab23e218d403069cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/820 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 820\n",
            "Correct records: 779\n",
            "Accuracy: 95.00%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "dataset = dataset.with_format(\"torch\", columns=['id', 'image', 'question', 'options', 'GT'])\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_loader = DataLoader(dataset,)\n",
        "records = {}\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "\n",
        "def check_correctness(model_answer, GT):\n",
        "    if \"<AND>\" in GT:\n",
        "        # å¤„ç† AND é€»è¾‘\n",
        "        gt_answers = GT.split(\"<AND>\")\n",
        "        return all(answer.strip().lower() in model_answer.lower() for answer in gt_answers)\n",
        "    elif \"<OR>\" in GT:\n",
        "        # å¤„ç† OR é€»è¾‘\n",
        "        gt_answers = GT.split(\"<OR>\")\n",
        "        return any(answer.strip().lower() in model_answer.lower() for answer in gt_answers)\n",
        "    return GT.strip().lower() in model_answer.lower()\n",
        "\n",
        "for batch in tqdm(data_loader):\n",
        "    id = batch['id'][0]\n",
        "    img = batch['image']  # shape: (B, C, H, W)\n",
        "\n",
        "    img_pil = to_pil_image(img[0])  # shape: [4, H, W]\n",
        "\n",
        "    question = batch['question'][0]\n",
        "    options = batch['options'][0]\n",
        "    full_prompt = f\"Question: {question}\\nOptions: {options}. Please only retuen a correct option without analysis.\"\n",
        "    GT = batch['GT'][0]\n",
        "    # print()\n",
        "    # print(full_prompt)\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img_pil},\n",
        "                {\"type\": \"text\", \"text\": full_prompt}\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    try:\n",
        "        response = pipe(text=messages)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {id}: {e}\")\n",
        "        raise e\n",
        "    model_answer = response[0][\"generated_text\"][-1][\"content\"]\n",
        "    whether_correct = check_correctness(model_answer, GT)\n",
        "    # record all of information for later analysis\n",
        "    records[id] = {\n",
        "        \"question\": full_prompt,\n",
        "        \"GT\": GT,\n",
        "        \"model_answer\": model_answer,\n",
        "        \"correct\": whether_correct\n",
        "    }\n",
        "# ä¿å­˜ç»“æœ\n",
        "print(f\"Total records: {len(records)}\")\n",
        "print(f\"Correct records: {sum(1 for r in records.values() if r['correct'])}\")\n",
        "print(f\"Accuracy: {sum(1 for r in records.values() if r['correct']) / len(records):.2%}\")\n",
        "import json\n",
        "with open(\"whats_up.json\", \"w\") as f:\n",
        "    json.dump(records, f, indent=2, ensure_ascii=False)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CV-Bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9675c9a79b3f4d57ab4b657fd2c4d82f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1803cf63968a4a2ca63227580ac1125c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "cv_bench-00000-of-00001.parquet:   0%|          | 0.00/221M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "599873425aeb44719c387f45d7260f41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating seed_bench_spatial split:   0%|          | 0/1635 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4963796bf4a45b8966b45abf5ed41dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating EgoOrientBench split:   0%|          | 0/33460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a92bcd995104fc7944d44b28993e5ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating GeoMeter split:   0%|          | 0/25557 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d66bd3d65dc04d808bfe5bacfa13b1cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating mm_vet split:   0%|          | 0/75 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfa417c34cec4cf1bf697e51be73c60e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating whats_up split:   0%|          | 0/820 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59c10f22c2b34cb2a17d3a12a463b73a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating cv_bench split:   0%|          | 0/2638 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'image', 'question', 'options', 'GT'],\n",
              "    num_rows: 2638\n",
              "})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "# Load a specific split \n",
        "cv_bench = load_dataset(\"LLDDSS/Awesome_Spatial_VLMs\", split=\"cv_bench\")\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "cv_bench\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['motorcycle', 'bus']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"choices\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2638/2638 [00:08<00:00, 315.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'image', 'question', 'options', 'GT'],\n",
            "    num_rows: 2638\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "cv_bench = load_dataset(\"nyu-visionx/CV-Bench\")\n",
        "\n",
        "# å‡è®¾åŸå§‹æ•°æ®é›†ä¸º raw_dataset\n",
        "raw_dataset = cv_bench[\"test\"]  # DatasetDict({'test': Dataset(...)})\n",
        "new_data = []\n",
        "mapping = {\"A\":0, \"B\":1, \"C\":2, \"D\":3, \"E\":4, \"F\":5, \"G\":6, \"H\":7, \"I\":8, \"J\":9}\n",
        "\n",
        "for example in tqdm(raw_dataset):\n",
        "    answer = example[\"answer\"][1]\n",
        "    choices = example[\"choices\"]\n",
        "    GT = choices[mapping[answer]]  # è·å–æ­£ç¡®ç­”æ¡ˆ\n",
        "\n",
        "    try:\n",
        "        new_data.append({\n",
        "            \"id\": str(example[\"idx\"]),\n",
        "            \"image\": example[\"image\"],\n",
        "            \"question\": example[\"prompt\"],\n",
        "            \"options\": '; '.join(example[\"choices\"]),\n",
        "            \"GT\": example[\"answer\"][1]\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error at idx {example['idx']}: {e}\")\n",
        "        continue\n",
        "\n",
        "new_dataset = Dataset.from_list(new_data)\n",
        "print(new_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Estimate the real-world distances between objects in this image. Which object is closer to the traffic cone (highlighted by a red box), the motorcycle (highlighted by a blue box) or the bus (highlighted by a green box)?\\n(A) motorcycle\\n(B) bus'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be67168cacc14556b94f2d7f9a8e0566",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb581b2b82964d199ce1fb7e124a4176",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2638 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "698e838d74eb4a4ba0f8d28700fa6503",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/27 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8a208e912014a78a8b13cd3cb7db579",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/datasets/LLDDSS/Awesome_Spatial_VLMs/commit/33956dcb9c1ea2ab58849fd8e9341e97ea47cb37', commit_message='Upload dataset', commit_description='', oid='33956dcb9c1ea2ab58849fd8e9341e97ea47cb37', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/LLDDSS/Awesome_Spatial_VLMs', endpoint='https://huggingface.co', repo_type='dataset', repo_id='LLDDSS/Awesome_Spatial_VLMs'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# upload to LLDDSS/Awesome_Spatial_VLMs\n",
        "new_dataset.push_to_hub(\"LLDDSS/Awesome_Spatial_VLMs\", split=\"cv_bench\")   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74a7d161644a417da1dae74a38d216c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0e4e706f0cf4fe5b6684443c423b625",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "cv_bench-00000-of-00001.parquet:   0%|          | 0.00/221M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29b55155d6c343bd85aca72eeadc580d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating seed_bench_spatial split:   0%|          | 0/1635 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "968e29e3bc3f4c89b7059b5eaafa3fa1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating EgoOrientBench split:   0%|          | 0/33460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fff5a3c1cb94edbb704a025c6b62cb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating GeoMeter split:   0%|          | 0/25557 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "232f44add6504baf8f36af4a4fa0a894",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating mm_vet split:   0%|          | 0/75 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c8fd484cba34b4e9e048f58909a2bcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating whats_up split:   0%|          | 0/820 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a723f8717964df482b0d3cffa8da17f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating cv_bench split:   0%|          | 0/2638 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a31fa7968d594abfb4de722ac00a58cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2638/2638 [22:37<00:00,  1.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 2638\n",
            "Correct records: 2004\n",
            "Accuracy: 75.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset = load_dataset(\"LLDDSS/Awesome_Spatial_VLMs\", split=\"cv_bench\")\n",
        "dataset = dataset.with_format(\"torch\", columns=['id', 'image', 'question', 'options', 'GT'])\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_loader = DataLoader(dataset,)\n",
        "records = {}\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "\n",
        "def check_correctness(model_answer, GT):\n",
        "    if \"<AND>\" in GT:\n",
        "        # å¤„ç† AND é€»è¾‘\n",
        "        gt_answers = GT.split(\"<AND>\")\n",
        "        return all(answer.strip().lower() in model_answer.lower() for answer in gt_answers)\n",
        "    elif \"<OR>\" in GT:\n",
        "        # å¤„ç† OR é€»è¾‘\n",
        "        gt_answers = GT.split(\"<OR>\")\n",
        "        return any(answer.strip().lower() in model_answer.lower() for answer in gt_answers)\n",
        "    return GT.strip().lower() in model_answer.lower()\n",
        "\n",
        "for batch in tqdm(data_loader):\n",
        "    id = batch['id'][0]\n",
        "    img = batch['image']  # shape: (B, C, H, W)\n",
        "\n",
        "    img_pil = to_pil_image(img[0])  # shape: [4, H, W]\n",
        "\n",
        "    question = batch['question'][0]\n",
        "    full_prompt = f\"Question: {question}.\\n Please only retuen a correct option without analysis.\"\n",
        "    GT = batch['GT'][0]\n",
        "    # print()\n",
        "    # print(full_prompt)\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img_pil},\n",
        "                {\"type\": \"text\", \"text\": full_prompt}\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    try:\n",
        "        response = pipe(text=messages)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {id}: {e}\")\n",
        "        raise e\n",
        "    model_answer = response[0][\"generated_text\"][-1][\"content\"]\n",
        "    whether_correct = check_correctness(model_answer, GT)\n",
        "    # record all of information for later analysis\n",
        "    records[id] = {\n",
        "        \"question\": full_prompt,\n",
        "        \"GT\": GT,\n",
        "        \"model_answer\": model_answer,\n",
        "        \"correct\": whether_correct\n",
        "    }\n",
        "# ä¿å­˜ç»“æœ\n",
        "print(f\"Total records: {len(records)}\")\n",
        "print(f\"Correct records: {sum(1 for r in records.values() if r['correct'])}\")\n",
        "print(f\"Accuracy: {sum(1 for r in records.values() if r['correct']) / len(records):.2%}\")\n",
        "import json\n",
        "with open(\"cv_bench.json\", \"w\") as f:\n",
        "    json.dump(records, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## srbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50b126ffdd4440ddadd6b1e23bf76703",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/390 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9f5d1d12d2a4df0ac8ab932f56265f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/188M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3961f880b3f24dc88f2fad4c6c5b00da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/1800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = load_dataset(\"stogian/srbench\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 900, 1600])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch['image'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': ['2637'],\n",
              " 'image': tensor([[[[ 55,  56,  56,  ...,  95,  95,  95],\n",
              "           [ 49,  51,  51,  ...,  85,  85,  85],\n",
              "           [ 47,  48,  49,  ...,  82,  82,  83],\n",
              "           ...,\n",
              "           [ 43,  89,  90,  ...,  94,  94,  94],\n",
              "           [ 43,  89,  90,  ...,  94,  94,  94],\n",
              "           [ 43,  89,  91,  ...,  94,  94,  94]],\n",
              " \n",
              "          [[ 60,  61,  61,  ..., 101, 101, 101],\n",
              "           [ 54,  56,  56,  ...,  91,  90,  91],\n",
              "           [ 52,  53,  54,  ...,  87,  87,  88],\n",
              "           ...,\n",
              "           [ 40,  86,  87,  ...,  98,  98,  98],\n",
              "           [ 40,  86,  87,  ...,  98,  98,  98],\n",
              "           [ 40,  86,  88,  ...,  98,  98,  98]],\n",
              " \n",
              "          [[ 64,  65,  65,  ..., 101, 101, 101],\n",
              "           [ 58,  60,  60,  ...,  91,  93,  91],\n",
              "           [ 56,  57,  58,  ...,  90,  91,  91],\n",
              "           ...,\n",
              "           [ 35,  79,  80,  ...,  97,  97,  97],\n",
              "           [ 35,  79,  80,  ...,  97,  97,  97],\n",
              "           [ 35,  79,  81,  ...,  97,  97,  97]]]], dtype=torch.uint8),\n",
              " 'question': ['Estimate the real-world distances between objects in this image. Which object is closer to the traffic cone (highlighted by a red box), the motorcycle (highlighted by a blue box) or the bus (highlighted by a green box)?\\n(A) motorcycle\\n(B) bus'],\n",
              " 'options': ['motorcycle; bus'],\n",
              " 'GT': ['A']}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'image', 'answer', 'split'],\n",
              "    num_rows: 1800\n",
              "})"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'This image shows a 3D polycube shape. Which of the options is simply the original shape in a rotated orientation?\\nOnly one of the options is correct.\\nAvailable options: A. Left, B. Center, C. Right',\n",
              " 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=286x257>,\n",
              " 'answer': 'B',\n",
              " 'split': 'mrt_easy'}"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc3676c6321f4a1baeb109ed1ab09d9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1800/1800 [10:20<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 1800\n",
            "Correct records: 947\n",
            "Accuracy: 52.61%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"stogian/srbench\", split=\"train\")\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_loader = DataLoader(dataset)\n",
        "records = {}\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"remyxai/SpaceOm\")\n",
        "\n",
        "def process_pil_image(pil_img, max_width=512):\n",
        "    # è½¬æ¢ä¸º RGB æ¨¡å¼ï¼ˆå› ä¸º SpaceOm æ¨¡å‹ä¸æ”¯æŒ RGBAï¼‰\n",
        "    img = pil_img.convert(\"RGB\")\n",
        "    # é™åˆ¶å®½åº¦ï¼Œä¿æŒçºµæ¨ªæ¯”ï¼ˆå¯é€‰ï¼‰\n",
        "    if img.width > max_width:\n",
        "        ratio = img.height / img.width\n",
        "        img = img.resize((max_width, int(max_width * ratio)), Image.Resampling.LANCZOS)\n",
        "    return img\n",
        "\n",
        "def check_correctness(model_answer, GT):\n",
        "    if \"<AND>\" in GT:\n",
        "        # å¤„ç† AND é€»è¾‘\n",
        "        gt_answers = GT.split(\"<AND>\")\n",
        "        return all(answer.strip().lower() in model_answer.lower() for answer in gt_answers)\n",
        "    elif \"<OR>\" in GT:\n",
        "        # å¤„ç† OR é€»è¾‘\n",
        "        gt_answers = GT.split(\"<OR>\")\n",
        "        return any(answer.strip().lower() in model_answer.lower() for answer in gt_answers)\n",
        "    return GT.strip().lower() in model_answer.lower()\n",
        "id = 0\n",
        "for batch in tqdm(dataset):\n",
        "    img = batch['image']  # shape: (B, C, H, W)\n",
        "    img_pil = process_pil_image(img)  # shape: [4, H, W]\n",
        "\n",
        "    question = batch['question']\n",
        "    full_prompt = f\"Question: {question}.\\n Please only retuen a correct option without analysis.\"\n",
        "    GT = batch['answer']\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": img_pil},\n",
        "                {\"type\": \"text\", \"text\": full_prompt}\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    try:\n",
        "        response = pipe(text=messages)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {id}: {e}\")\n",
        "        raise e\n",
        "    model_answer = response[0][\"generated_text\"][-1][\"content\"]\n",
        "    whether_correct = check_correctness(model_answer, GT)\n",
        "    # record all of information for later analysis\n",
        "    records[id] = {\n",
        "        \"question\": full_prompt,\n",
        "        \"GT\": GT,\n",
        "        \"model_answer\": model_answer,\n",
        "        \"correct\": whether_correct\n",
        "    }\n",
        "    id += 1\n",
        "print(f\"Total records: {len(records)}\")\n",
        "print(f\"Correct records: {sum(1 for r in records.values() if r['correct'])}\")\n",
        "print(f\"Accuracy: {sum(1 for r in records.values() if r['correct']) / len(records):.2%}\")\n",
        "import json\n",
        "with open(\"srbench.json\", \"w\") as f:\n",
        "    json.dump(records, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
