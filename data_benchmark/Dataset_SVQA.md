## ðŸ“Š Spatial Training Datasets

<div style="overflow-x:auto;">
<table style="border-collapse:collapse;font-size:13px;width:100%;">
<thead><tr style="background-color:#f5f5f5;"><th style="border:1px solid #ccc;padding:4px 6px;text-align:center;">Dataset</th><th style="border:1px solid #ccc;padding:4px 6px;text-align:center;">Venue</th><th style="border:1px solid #ccc;padding:4px 6px;text-align:center;">P.</th><th style="border:1px solid #ccc;padding:4px 6px;text-align:center;">U.</th><th style="border:1px solid #ccc;padding:4px 6px;text-align:center;">E.</th><th style="border:1px solid #ccc;padding:4px 6px;text-align:center;">Fundamental Task</th><th style="border:1px solid #ccc;padding:4px 6px;text-align:center;">Size</th><th style="border:1px solid #ccc;padding:4px 6px;text-align:center;">Image Source</th><th style="border:1px solid #ccc;padding:4px 6px;text-align:center;">Modality</th></tr></thead><tbody>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/Electronics/ProximityQA/blob/main/llava_proximity-mix.json">Proximity-110K</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ArXiv2024</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">depth estimation</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">989,877</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Visual Genome, COCO</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/OpenGVLab/AS-V2">AS-V2</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ECCV2024</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Spatial Relations VQA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">127,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">COCO</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/remyxai/SpaceThinker">SpaceThinker</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">online</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Spatial Relations VQA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">12,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">VQASynth</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/a8cheng/OpenSpatialDataset">OpenSpatial</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">NeurIPS2024</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Spatial Relations VQA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">8,700,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">OpenImages</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB-D</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://arpg.github.io/sunspot/">SUN-Spot v2.0</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ArXiv2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Spatial Relations VQA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">101,053</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">SUN RGB-D</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB-D</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://github.com/SilongYong/SQA3D">SQA3D</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ICLR2023</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Spatial Situated Reasoning</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">33,400</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ScanNet</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB, Point Cloud</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://docs.google.com/forms/d/e/1FAIpQLSdzj25kXcthCKtypAI0wQP8j16e9F7ODBroL4SCH_ly8_3rKw/viewform">MSQA</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">NeurIPS2024</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Spatial Situated Reasoning</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">251,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ScanNet , 3RScan , ARKitScenes</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Point Cloud</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://github.com/zhangyuejoslin/Spartun3D?tab=readme-ov-file">Spartun3D</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ICLR2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Spatial Situated Reasoning</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">123,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">3RScan</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Point Cloud</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/WanyueZhang/MulSeT">MulSeT</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ArXiv2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Spatial Situated Reasoning, Spatial Simulation and Inferring</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">38,200</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">AI2THOR</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://github.com/XingruiWang/superclevr-3D-question">Super-CLEVR-3D(Pose, occlusion) </a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">NeurIPS2023</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Orientation Estimation, Spatial relation VQA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">543,383</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Super-CLEVR</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/RussRobin/SpatialQA">SpatialQA</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ICRA2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Depth estimation, 3D object detection, Spatial relation VQA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">852,869</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Bunny 695k, Open X-Embodiment</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB-D</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/bonbon-rj/SURDS">SURDS</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ArXiv2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Depth estimation, Orientation estiamtion, Spatial Relations VQA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">50,330</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">nuScenes</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://drive.google.com/drive/folders/124lDTAukZi6zCtpwTjxu6y3n8z5B2E8v">Open3DVQA</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ArXiv2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">3D Object Detection, Orientation estimation, Spatial Relations VQA,</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">9,048</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">synthetic data</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB-D</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/JingkunAn/RefSpatial">RefSpatial</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">NeurIPS2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">3D Object Detection, Depth estimation, Spatial relation QA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">22,000,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">OpenImages, CA-1M, synthetic</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB-D</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/yliu-cs/SSR-CoT/">SSR-CoT</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">NeurIPS2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">3D Object Detection, Depth estimation, Spatial relation QA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">1,200,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">LLaVA-CoT, Visual-CoT, VoCoT, SpatialQA</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB-D</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/RUBBISHLIKE/SpaceR-151k">SR-91k</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ArXiv2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Depth estimation, Object Detection, Spatial Situated Reasoning</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">91,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ScanNet</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/Endlinc/SpaceSGG">SpaceSGG</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">WACV2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Spatial Relation VQA, Spatial Situated Reasoning</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">40,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">COCO</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/array/SAT">SAT</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ArXiv2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Depth estimation,Spatial relations VQA, Spatial situated reasoning, Spatial Simulation and Inferring</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">175,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">PixMo-Cap, DOCCI, Pixmo-Cap</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/jasonzhango/SPAR-7M">SPAR-7M</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ArXiv2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Depth estimation, Spatial relations VQA, Spatial simulation and inferring, Spatial situated reasoning</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">7,000,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ProcTHOR-10K</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
<tr style="background-color:#fafafa;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://github.com/NVlabs/RoboSpatial?tab=readme-ov-file">RoboSpatial</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">CVPR2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">3D Object Detection, Spatial Relations VQA, Spatial Situated Reasoning, Spatial Simulation and Inferring</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">3,000,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">Matterport3D, ScanNet, 3RScan, HOPE, GraspNet-1B</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB, Point Cloud</td></tr>
<tr style="background-color:#ffffff;"><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;"><a href="https://huggingface.co/datasets/cpystan/MSMU">MSMU</a></td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">NeurIPS2025</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">âœ“</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">3D Object Detection, Spatial Relations VQA, Spatial Simulation and Inferring</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">700,000</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">ScanNet, ScanNet++,</td><td style="border:1px solid #ddd;padding:3px 5px;text-align:center;">RGB</td></tr>
</tbody></table></div>

> Total: **22 datasets Ã— 9 columns**
